# STTms - Speech-to-Text Microservice

üé§ **Servicio de transcripci√≥n de audio de alta velocidad con aceleraci√≥n por GPU**

STTms es un microservicio especializado en transcripci√≥n de audio que utiliza OpenAI Whisper, ejecutado en local y con soporte para GPU NVIDIA. Dise√±ado para integrarse con sistemas de asistentes de voz y aplicaciones que requieren transcripci√≥n en tiempo real con alta precisi√≥n.

## üöÄ Caracter√≠sticas principales

- ‚úÖ **Transcripci√≥n con GPU**: Aceleraci√≥n CUDA para procesamiento r√°pido
- ‚úÖ **M√∫ltiples idiomas**: Soporte para espa√±ol (por defecto) y otros idiomas
- ‚úÖ **API REST**: Interfaz HTTP simple y eficiente
- ‚úÖ **Debug integrado**: Guardado y reproducci√≥n de audio para depuraci√≥n
- ‚úÖ **Docker optimizado**: Contenedor con PyTorch y CUDA preinstalados
- ‚úÖ **Configuraci√≥n flexible**: Variables de entorno para personalizaci√≥n
- ‚úÖ **Logs detallados**: Monitoreo completo del proceso de transcripci√≥n

## üõ†Ô∏è Requisitos del sistema

### Hardware m√≠nimo
- **GPU**: NVIDIA con soporte CUDA 11.8+ (recomendado GTX 1060 o superior)
- **RAM**: 4GB m√≠nimo, 8GB recomendado
- **Almacenamiento**: 2GB para modelos base, 5GB para modelos grandes

### Software
- **Docker** 20.10+
- **Docker Compose** 2.0+
- **NVIDIA Container Toolkit**
- **Drivers NVIDIA** compatibles con CUDA 11.8

## üì¶ Instalaci√≥n

### 1. Clonar el repositorio
```bash
git clone <tu-repositorio>/STTms.git
cd STTms
```

### 2. Configurar variables de entorno
Crea un archivo `.env` en la ra√≠z del proyecto:

```bash
# Configuraci√≥n de la API
API_PORT=5000
FLASK_HOST=0.0.0.0
FLASK_PORT=5000

# Configuraci√≥n de GPU
NVIDIA_VISIBLE_DEVICES=all

# Configuraci√≥n de Whisper
WHISPER_MODEL=base              # tiny, base, small, medium, large
DEFAULT_LANGUAGE=es             # Idioma por defecto (espa√±ol)

# Debug (opcional)
DEBUG_AUDIO=true               # Guardar audio para debug
```

### 3. Construir y ejecutar
```bash
# Construir e iniciar el servicio
docker-compose up --build

# Ejecutar en segundo plano
docker-compose up -d --build
```

## üîß Configuraci√≥n

### Modelos disponibles
| Modelo | Tama√±o | VRAM | Velocidad | Precisi√≥n |
|--------|--------|------|-----------|-----------|
| `tiny` | 39 MB | ~1GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê |
| `base` | 74 MB | ~1GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê |
| `small` | 244 MB | ~2GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `medium` | 769 MB | ~5GB | üêå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `large` | 1550 MB | ~10GB | üêåüêå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### Idiomas soportados
- `es` - Espa√±ol (por defecto)
- `en` - Ingl√©s
- `fr` - Franc√©s
- `de` - Alem√°n
- `it` - Italiano
- `pt` - Portugu√©s
- Y muchos m√°s...

## üì° Uso del API

### Endpoint principal: `/transcribe`

**Transcripci√≥n b√°sica (espa√±ol por defecto)**
```bash
curl -X POST \
  -F "audio=@audio.wav" \
  http://localhost:5000/transcribe
```

**Especificando idioma**
```bash
curl -X POST \
  -F "audio=@audio.wav" \
  -F "language=en" \
  http://localhost:5000/transcribe
```

**Respuesta**
```json
{
  "transcription": "enciende led verde",
  "language": "es",
  "detected_language": "es",
  "debug_audio_file": "audio_20241220_143055_123.wav",
  "debug_audio_url": "/debug/audio/audio_20241220_143055_123.wav"
}
```

### Endpoints de debug

**Listar archivos de audio guardados**
```bash
curl http://localhost:5000/debug/audio
```

**Descargar archivo de audio espec√≠fico**
```bash
curl http://localhost:5000/debug/audio/audio_20241220_143055_123.wav -o test.wav
```

## üêõ Funcionalidades de debug

### Guardado autom√°tico de audio
Cuando `DEBUG_AUDIO=true`, cada audio recibido se guarda autom√°ticamente en `./debug_audio/` con formato:
```
audio_YYYYMMDD_HHMMSS_mmm.wav
```

### Casos de uso del debug
1. **Verificar calidad del audio**: Reproduce el archivo guardado
2. **Comparar transcripciones**: Prueba el mismo audio con diferentes modelos
3. **Analizar problemas**: Identifica si el problema est√° en el audio o el modelo
4. **Optimizaci√≥n**: Ajusta configuraciones bas√°ndote en resultados reales

### Ejemplo de debug completo
```bash
# 1. Hacer transcripci√≥n
curl -X POST -F "audio=@mi_audio.wav" http://localhost:5000/transcribe

# 2. Ver archivos guardados
curl http://localhost:5000/debug/audio

# 3. Descargar para escuchar
curl http://localhost:5000/debug/audio/audio_20241220_143055_123.wav -o debug.wav

# 4. Reproducir el audio
vlc debug.wav  # o mpv debug.wav
```

## üîç Integraci√≥n con clientes

### Python
```python
import requests

def transcribir_audio(archivo_audio, idioma="es"):
    url = "http://localhost:5000/transcribe"
    
    with open(archivo_audio, 'rb') as f:
        files = {"audio": f}
        data = {"language": idioma}
        
        response = requests.post(url, files=files, data=data)
        return response.json()

# Uso
resultado = transcribir_audio("audio.wav", "es")
print(f"Transcripci√≥n: {resultado['transcription']}")
```

### JavaScript/Node.js
```javascript
const FormData = require('form-data');
const fs = require('fs');
const axios = require('axios');

async function transcribirAudio(archivoAudio, idioma = 'es') {
    const form = new FormData();
    form.append('audio', fs.createReadStream(archivoAudio));
    form.append('language', idioma);
    
    const response = await axios.post('http://localhost:5000/transcribe', form, {
        headers: form.getHeaders()
    });
    
    return response.data;
}

// Uso
transcribirAudio('audio.wav', 'es')
    .then(resultado => console.log('Transcripci√≥n:', resultado.transcription));
```

## üìä Monitoreo y logs

### Logs del contenedor
```bash
# Ver logs en tiempo real
docker-compose logs -f whisper-api

# Ver √∫ltimos logs
docker-compose logs --tail=100 whisper-api
```

### Informaci√≥n de GPU
```bash
# Verificar uso de GPU
nvidia-smi

# Ver logs de GPU en el contenedor
docker exec whisper-api nvidia-smi
```

## ‚ö° Optimizaci√≥n de rendimiento

### Para GPU de gama baja
```bash
WHISPER_MODEL=tiny
DEBUG_AUDIO=false
```

### Para m√°xima precisi√≥n
```bash
WHISPER_MODEL=large
DEBUG_AUDIO=true
```

### Para balance rendimiento/precisi√≥n
```bash
WHISPER_MODEL=base  # o small
DEBUG_AUDIO=true
```

## üö® Troubleshooting

### Problemas comunes

**Error: CUDA not available**
```bash
# Verificar instalaci√≥n de NVIDIA Container Toolkit
sudo apt-get install nvidia-container-toolkit
sudo systemctl restart docker
```

**Audio no se transcribe correctamente**
1. Verificar calidad del audio usando debug
2. Probar con modelo m√°s grande
3. Especificar idioma correcto
4. Verificar formato de audio (preferible WAV 16kHz mono)

**Contenedor no inicia**
```bash
# Verificar logs detallados
docker-compose logs whisper-api

# Verificar recursos de GPU disponibles
nvidia-smi
```

**Respuesta lenta**
1. Usar modelo m√°s peque√±o (`tiny` o `base`)
2. Verificar que la GPU est√© siendo utilizada
3. Reducir calidad del audio de entrada

## üîí Seguridad

- Los archivos de audio se eliminan autom√°ticamente despu√©s de la transcripci√≥n
- Los archivos de debug se guardan localmente (no se transmiten)
- El servicio no almacena transcripciones permanentemente
- Configurar firewall para limitar acceso al puerto 5000

## ü§ù Integraci√≥n con puertocho-assistant

Este servicio est√° dise√±ado para integrarse con el sistema puertocho-assistant:

```python
# Ejemplo de integraci√≥n
audio_data = grabar_audio()
response = requests.post(
    "http://whisper-api:5000/transcribe",
    files={"audio": audio_data},
    data={"language": "es"}
)
comando = response.json()["transcription"]
```

## üìà M√©tricas y estad√≠sticas

El servicio proporciona informaci√≥n √∫til en cada respuesta:
- Idioma configurado vs. detectado
- Tiempo de procesamiento (en logs)
- Calidad del modelo usado
- Archivo de debug generado

## üõ£Ô∏è Roadmap

- [ ] Soporte para streaming de audio en tiempo real
- [ ] M√©tricas de rendimiento integradas
- [ ] Soporte para m√∫ltiples formatos de audio
- [ ] Cache de modelos para inicio m√°s r√°pido
- [ ] API de administraci√≥n para cambiar modelos din√°micamente

## üìÑ Licencia

Este proyecto utiliza las siguientes tecnolog√≠as:
- **OpenAI Whisper**: MIT License
- **PyTorch**: BSD License
- **Flask**: BSD License

## üÜò Soporte

Para reportar problemas o solicitar caracter√≠sticas:
1. Verificar la secci√≥n de troubleshooting
2. Revisar los logs del contenedor
3. Usar las funcionalidades de debug
4. Crear un issue con informaci√≥n detallada

---

**Desarrollado con ‚ù§Ô∏è para el ecosistema puertocho-assistant** 