#!/usr/bin/env python3
"""
Script de prueba para el sistema de confidence scoring avanzado.
Prueba las 10 mÃ©tricas implementadas y verifica la configuraciÃ³n dinÃ¡mica.
"""

import requests
import json
import time
from typing import Dict, Any

class ConfidenceScoringTester:
    def __init__(self, base_url: str = "http://localhost:8080"):
        self.base_url = base_url
        self.api_base = f"{base_url}/api/v1/rag-classifier"
        
    def test_basic_classification(self, text: str) -> Dict[str, Any]:
        """Prueba clasificaciÃ³n bÃ¡sica"""
        print(f"\nğŸ” Probando clasificaciÃ³n bÃ¡sica: '{text}'")
        
        try:
            response = requests.post(
                f"{self.api_base}/classify",
                params={"text": text},
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                print(f"âœ… ClasificaciÃ³n exitosa:")
                print(f"   - Intent: {result.get('intent_id')}")
                print(f"   - Confidence: {result.get('confidence_score', 0):.3f}")
                print(f"   - Processing time: {result.get('processing_time_ms', 0)}ms")
                print(f"   - Fallback used: {result.get('fallback_used', False)}")
                return result
            else:
                print(f"âŒ Error en clasificaciÃ³n: {response.status_code}")
                print(f"   Response: {response.text}")
                return {}
                
        except Exception as e:
            print(f"âŒ ExcepciÃ³n en clasificaciÃ³n: {e}")
            return {}
    
    def test_advanced_classification(self, text: str, session_id: str = None) -> Dict[str, Any]:
        """Prueba clasificaciÃ³n avanzada con metadata"""
        print(f"\nğŸ” Probando clasificaciÃ³n avanzada: '{text}'")
        
        request_data = {
            "text": text,
            "sessionId": session_id or f"test_session_{int(time.time())}",
            "userId": "test_user",
            "maxExamplesForRag": 5,
            "confidenceThreshold": 0.7,
            "contextMetadata": {
                "source": "test_script",
                "timestamp": int(time.time()),
                "test_type": "confidence_scoring"
            }
        }
        
        try:
            response = requests.post(
                f"{self.api_base}/classify/advanced",
                json=request_data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                print(f"âœ… ClasificaciÃ³n avanzada exitosa:")
                print(f"   - Intent: {result.get('intent_id')}")
                print(f"   - Confidence: {result.get('confidence_score', 0):.3f}")
                print(f"   - Processing time: {result.get('processing_time_ms', 0)}ms")
                print(f"   - Vector search time: {result.get('vector_search_time_ms', 0)}ms")
                print(f"   - LLM inference time: {result.get('llm_inference_time_ms', 0)}ms")
                print(f"   - Fallback used: {result.get('fallback_used', False)}")
                return result
            else:
                print(f"âŒ Error en clasificaciÃ³n avanzada: {response.status_code}")
                print(f"   Response: {response.text}")
                return {}
                
        except Exception as e:
            print(f"âŒ ExcepciÃ³n en clasificaciÃ³n avanzada: {e}")
            return {}
    
    def test_confidence_metrics(self, text: str) -> Dict[str, Any]:
        """Prueba el endpoint de mÃ©tricas detalladas de confidence"""
        print(f"\nğŸ” Probando mÃ©tricas detalladas de confidence: '{text}'")
        
        request_data = {
            "text": text,
            "sessionId": f"metrics_test_{int(time.time())}",
            "userId": "test_user",
            "maxExamplesForRag": 5,
            "confidenceThreshold": 0.7,
            "contextMetadata": {
                "source": "confidence_metrics_test",
                "timestamp": int(time.time())
            }
        }
        
        try:
            response = requests.post(
                f"{self.api_base}/confidence-metrics",
                json=request_data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                print(f"âœ… MÃ©tricas de confidence obtenidas:")
                print(f"   - Intent: {result.get('intent_id')}")
                print(f"   - Final confidence: {result.get('final_confidence', 0):.3f}")
                print(f"   - Processing time: {result.get('processing_time_ms', 0)}ms")
                
                # Mostrar mÃ©tricas detalladas
                metrics = result.get('confidence_metrics', {})
                if metrics:
                    print(f"   ğŸ“Š MÃ©tricas detalladas:")
                    for metric, value in metrics.items():
                        print(f"      - {metric}: {value:.3f}")
                
                return result
            else:
                print(f"âŒ Error obteniendo mÃ©tricas: {response.status_code}")
                print(f"   Response: {response.text}")
                return {}
                
        except Exception as e:
            print(f"âŒ ExcepciÃ³n obteniendo mÃ©tricas: {e}")
            return {}
    
    def test_statistics(self) -> Dict[str, Any]:
        """Prueba el endpoint de estadÃ­sticas"""
        print(f"\nğŸ” Obteniendo estadÃ­sticas del motor RAG")
        
        try:
            response = requests.get(f"{self.api_base}/statistics", timeout=30)
            
            if response.status_code == 200:
                stats = response.json()
                print(f"âœ… EstadÃ­sticas obtenidas:")
                print(f"   - Total classifications: {stats.get('total_classifications', 0)}")
                print(f"   - Average confidence: {stats.get('average_confidence', 0):.3f}")
                print(f"   - Average processing time: {stats.get('average_processing_time_ms', 0)}ms")
                print(f"   - Fallback rate: {stats.get('fallback_rate', 0):.3f}")
                return stats
            else:
                print(f"âŒ Error obteniendo estadÃ­sticas: {response.status_code}")
                return {}
                
        except Exception as e:
            print(f"âŒ ExcepciÃ³n obteniendo estadÃ­sticas: {e}")
            return {}
    
    def test_health(self) -> Dict[str, Any]:
        """Prueba el endpoint de health check"""
        print(f"\nğŸ” Verificando salud del motor RAG")
        
        try:
            response = requests.get(f"{self.api_base}/health", timeout=30)
            
            if response.status_code == 200:
                health = response.json()
                status = health.get('status', 'UNKNOWN')
                print(f"âœ… Health check: {status}")
                
                components = health.get('components', {})
                for component, status in components.items():
                    print(f"   - {component}: {status}")
                
                return health
            else:
                print(f"âŒ Error en health check: {response.status_code}")
                return {}
                
        except Exception as e:
            print(f"âŒ ExcepciÃ³n en health check: {e}")
            return {}
    
    def run_comprehensive_test(self):
        """Ejecuta una prueba completa del sistema"""
        print("ğŸš€ INICIANDO PRUEBA COMPLETA DEL SISTEMA DE CONFIDENCE SCORING")
        print("=" * 70)
        
        # Test cases con diferentes niveles de complejidad
        test_cases = [
            "Â¿quÃ© tiempo hace en Madrid?",
            "enciende la luz del salÃ³n",
            "ayÃºdame con algo",
            "reproduce mÃºsica relajante",
            "programa una alarma para maÃ±ana",
            "texto muy ambiguo que no deberÃ­a tener alta confianza",
            "comando muy especÃ­fico y claro para el sistema"
        ]
        
        # Verificar salud del sistema
        health_result = self.test_health()
        if not health_result:
            print("âŒ Sistema no disponible, abortando pruebas")
            return
        
        # Obtener estadÃ­sticas iniciales
        initial_stats = self.test_statistics()
        
        # Probar cada caso de test
        results = []
        for i, text in enumerate(test_cases, 1):
            print(f"\nğŸ“ CASO DE PRUEBA {i}/{len(test_cases)}")
            print("-" * 50)
            
            # ClasificaciÃ³n bÃ¡sica
            basic_result = self.test_basic_classification(text)
            
            # ClasificaciÃ³n avanzada
            advanced_result = self.test_advanced_classification(text)
            
            # MÃ©tricas detalladas de confidence
            metrics_result = self.test_confidence_metrics(text)
            
            results.append({
                "text": text,
                "basic": basic_result,
                "advanced": advanced_result,
                "metrics": metrics_result
            })
            
            # Pausa entre pruebas
            time.sleep(1)
        
        # Obtener estadÃ­sticas finales
        final_stats = self.test_statistics()
        
        # Resumen de resultados
        print(f"\nğŸ“Š RESUMEN DE PRUEBAS")
        print("=" * 70)
        
        successful_tests = sum(1 for r in results if r['basic'] and r['advanced'])
        print(f"âœ… Pruebas exitosas: {successful_tests}/{len(test_cases)}")
        
        if successful_tests > 0:
            avg_confidence = sum(
                r['basic'].get('confidence_score', 0) for r in results if r['basic']
            ) / successful_tests
            print(f"ğŸ“ˆ Confidence promedio: {avg_confidence:.3f}")
            
            avg_processing_time = sum(
                r['basic'].get('processing_time_ms', 0) for r in results if r['basic']
            ) / successful_tests
            print(f"â±ï¸  Tiempo de procesamiento promedio: {avg_processing_time:.0f}ms")
        
        # Comparar estadÃ­sticas
        if initial_stats and final_stats:
            initial_classifications = initial_stats.get('total_classifications', 0)
            final_classifications = final_stats.get('total_classifications', 0)
            new_classifications = final_classifications - initial_classifications
            print(f"ğŸ”„ Nuevas clasificaciones durante la prueba: {new_classifications}")
        
        print(f"\nğŸ‰ PRUEBA COMPLETA FINALIZADA")

def main():
    """FunciÃ³n principal"""
    import sys
    
    # Configurar URL base
    base_url = sys.argv[1] if len(sys.argv) > 1 else "http://localhost:8080"
    
    print(f"ğŸ”§ Configurando pruebas para: {base_url}")
    
    # Crear tester y ejecutar pruebas
    tester = ConfidenceScoringTester(base_url)
    tester.run_comprehensive_test()

if __name__ == "__main__":
    main() 