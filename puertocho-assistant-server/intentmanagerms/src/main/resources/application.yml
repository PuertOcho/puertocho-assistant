spring:
  application:
    name: ${SPRING_APPLICATION_NAME:intent-manager}
  config:
    import: optional:configserver:http://${CONFIG_SERVICE:configms}:${PORT_CONFIG:8888}
  cloud:
    config:
      name: intent-manager
  data:
    redis:
      host: ${REDIS_HOST:redis}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD:}
      database: ${REDIS_DATABASE:0}
      timeout: ${REDIS_TIMEOUT:2000ms}
      lettuce:
        pool:
          max-active: ${REDIS_MAX_ACTIVE:8}
          max-idle: ${REDIS_MAX_IDLE:8}
          min-idle: ${REDIS_MIN_IDLE:0}
          max-wait: ${REDIS_MAX_WAIT:-1ms}

# Configuración del servidor
server:
  port: ${SERVER_PORT:9904}
  servlet:
    context-path: /

# Configuración de logging
logging:
  level:
    com.intentmanagerms: ${LOG_LEVEL:INFO}
    org.springframework.web: ${SPRING_LOG_LEVEL:INFO}
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"

# Configuración del motor RAG
rag:
  classifier:
    default-max-examples: ${RAG_DEFAULT_MAX_EXAMPLES:5}
    default-confidence-threshold: ${RAG_DEFAULT_CONFIDENCE_THRESHOLD:0.7}
    similarity-threshold: ${RAG_SIMILARITY_THRESHOLD:0.6}
    enable-fallback: ${RAG_ENABLE_FALLBACK:true}
    fallback-confidence-threshold: ${RAG_FALLBACK_CONFIDENCE_THRESHOLD:0.5}
    max-processing-time-ms: ${RAG_MAX_PROCESSING_TIME_MS:10000}
  confidence:
    weights:
      llm: ${RAG_CONFIDENCE_WEIGHT_LLM:0.25}
      similarity: ${RAG_CONFIDENCE_WEIGHT_SIMILARITY:0.20}
      consistency: ${RAG_CONFIDENCE_WEIGHT_CONSISTENCY:0.15}
      example-count: ${RAG_CONFIDENCE_WEIGHT_EXAMPLE_COUNT:0.10}
      semantic-diversity: ${RAG_CONFIDENCE_WEIGHT_SEMANTIC_DIVERSITY:0.10}
      temporal: ${RAG_CONFIDENCE_WEIGHT_TEMPORAL:0.05}
      embedding-quality: ${RAG_CONFIDENCE_WEIGHT_EMBEDDING_QUALITY:0.05}
      entropy: ${RAG_CONFIDENCE_WEIGHT_ENTROPY:0.05}
      contextual: ${RAG_CONFIDENCE_WEIGHT_CONTEXTUAL:0.03}
      prompt-robustness: ${RAG_CONFIDENCE_WEIGHT_PROMPT_ROBUSTNESS:0.02}
    thresholds:
      optimal-processing-time-ms: ${RAG_CONFIDENCE_OPTIMAL_PROCESSING_TIME_MS:500}
      max-processing-time-ms: ${RAG_CONFIDENCE_MAX_PROCESSING_TIME_MS:2000}
      min-examples: ${RAG_CONFIDENCE_MIN_EXAMPLES:2}
  similarity:
    search-algorithm: ${RAG_SIMILARITY_SEARCH_ALGORITHM:hybrid}
    diversity-threshold: ${RAG_SIMILARITY_DIVERSITY_THRESHOLD:0.3}
    intent-weight: ${RAG_SIMILARITY_INTENT_WEIGHT:0.7}
    content-weight: ${RAG_SIMILARITY_CONTENT_WEIGHT:0.3}
    enable-diversity-filtering: ${RAG_SIMILARITY_ENABLE_DIVERSITY_FILTERING:true}
    enable-intent-clustering: ${RAG_SIMILARITY_ENABLE_INTENT_CLUSTERING:true}
    max-cluster-size: ${RAG_SIMILARITY_MAX_CLUSTER_SIZE:3}
    enable-semantic-boosting: ${RAG_SIMILARITY_ENABLE_SEMANTIC_BOOSTING:true}
  prompt:
    strategy: ${RAG_PROMPT_STRATEGY:adaptive}
    max-context-length: ${RAG_PROMPT_MAX_CONTEXT_LENGTH:3000}
    enable-chain-of-thought: ${RAG_PROMPT_ENABLE_CHAIN_OF_THOUGHT:true}
    enable-contextual-hints: ${RAG_PROMPT_ENABLE_CONTEXTUAL_HINTS:true}
    enable-entity-extraction: ${RAG_PROMPT_ENABLE_ENTITY_EXTRACTION:true}
    enable-confidence-calibration: ${RAG_PROMPT_ENABLE_CONFIDENCE_CALIBRATION:true}
    temperature: ${RAG_PROMPT_TEMPERATURE:0.3}
    max-tokens: ${RAG_PROMPT_MAX_TOKENS:2048}
    language: ${RAG_PROMPT_LANGUAGE:es}
  fallback:
    enable-gradual-degradation: ${RAG_FALLBACK_ENABLE_GRADUAL_DEGRADATION:true}
    max-degradation-levels: ${RAG_FALLBACK_MAX_DEGRADATION_LEVELS:5}
    similarity-reduction-factor: ${RAG_FALLBACK_SIMILARITY_REDUCTION_FACTOR:0.2}
    enable-keyword-fallback: ${RAG_FALLBACK_ENABLE_KEYWORD_FALLBACK:true}
    enable-context-fallback: ${RAG_FALLBACK_ENABLE_CONTEXT_FALLBACK:true}
    enable-general-domain-fallback: ${RAG_FALLBACK_ENABLE_GENERAL_DOMAIN_FALLBACK:true}
    min-confidence-for-degradation: ${RAG_FALLBACK_MIN_CONFIDENCE_FOR_DEGRADATION:0.3}
    max-processing-time-ms: ${RAG_FALLBACK_MAX_PROCESSING_TIME_MS:5000}

# Configuración de LLMs
llm:
  primary:
    model: ${PRIMARY_LLM_MODEL:gpt-4}
    provider: ${PRIMARY_LLM_PROVIDER:openai}
    api-key: ${OPENAI_API_KEY:}
  timeout: ${LLM_TIMEOUT:30s}
  max-tokens: ${LLM_MAX_TOKENS:4096}
  temperature: ${LLM_TEMPERATURE:0.7}

# Configuración MoE
moe:
  enabled: ${MOE_ENABLED:true}
  llm-a:
    model: ${MOE_LLM_A_MODEL:gpt-4}
  llm-b:
    model: ${MOE_LLM_B_MODEL:claude-3-sonnet-20240229}
  llm-c:
    model: ${MOE_LLM_C_MODEL:gpt-3.5-turbo}

# Configuración de Vector Store
vector-store:
  type: ${VECTOR_STORE_TYPE:in-memory}
  collection-name: ${VECTOR_STORE_COLLECTION:intent-examples}
  embedding-dimension: ${VECTOR_STORE_EMBEDDING_DIMENSION:1536}
  max-results: ${VECTOR_STORE_MAX_RESULTS:10}
  similarity-threshold: ${VECTOR_STORE_SIMILARITY_THRESHOLD:0.7}
  initialize-with-examples: ${VECTOR_STORE_INIT_EXAMPLES:true}
  example-count: ${VECTOR_STORE_EXAMPLE_COUNT:5}

# Configuración de intenciones
intent:
  config:
    file: ${INTENT_CONFIG_FILE:classpath:config/intents.json}
    hot-reload:
      enabled: ${INTENT_HOT_RELOAD_ENABLED:true}
      interval: ${INTENT_HOT_RELOAD_INTERVAL:30}
    default-confidence-threshold: ${INTENT_DEFAULT_CONFIDENCE_THRESHOLD:0.7}
    default-max-examples-for-rag: ${INTENT_DEFAULT_MAX_EXAMPLES_FOR_RAG:5}

# Configuración de MCP Registry
mcp:
  registry:
    file: ${MCP_REGISTRY_FILE:classpath:config/mcp_registry.json}
    hot-reload:
      enabled: ${MCP_HOT_RELOAD_ENABLED:true}
      interval: ${MCP_HOT_RELOAD_INTERVAL:30}
    default-timeout: ${MCP_DEFAULT_TIMEOUT:30}
    default-retry-attempts: ${MCP_DEFAULT_RETRY_ATTEMPTS:3}
    default-health-check-interval: ${MCP_DEFAULT_HEALTH_CHECK_INTERVAL:60}

# Configuración de Eureka
eureka:
  client:
    service-url:
      defaultZone: ${EUREKA_SERVICE_URL:http://eurekams:8761/eureka/}
    register-with-eureka: ${EUREKA_REGISTER:true}
    fetch-registry: ${EUREKA_FETCH:true}
  instance:
    prefer-ip-address: true
    instance-id: ${spring.application.name}:${server.port}