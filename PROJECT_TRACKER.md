# PROJECT TRACKER ‚Äì IntentManagerMS LLM-RAG + MoE Implementation

> √öltima actualizaci√≥n: 2025-01-27 (Actualizado con implementaci√≥n real T1.1 y T1.2)
> 
> **Objetivo**: Implementaci√≥n completamente nueva de intentmanagerms usando arquitectura LLM-RAG + Mixture of Experts (MoE) para clasificaci√≥n de intenciones escalable, conversaci√≥n multivuelta inteligente y soporte nativo MCP.

---

## Leyenda de estados
- ‚úÖ Completado
- üîÑ En progreso  
- ‚è≥ Pendiente
- üöß Bloqueado

---

## Epic 1 ‚Äì Arquitectura Base y Configuraci√≥n LLM-RAG

**Descripci√≥n del Epic**: Establecer los fundamentos t√©cnicos del sistema LLM-RAG. Este Epic crea la infraestructura base necesaria para soportar m√∫ltiples LLMs, almacenamiento vectorial para RAG, configuraci√≥n din√°mica desde JSON, y el registro de acciones MCP. Es la base sobre la que se construyen todos los dem√°s Epics.

**Objetivos clave**: 
- Infraestructura Spring Boot funcional con LangChain4j
- Gesti√≥n configurable de m√∫ltiples LLMs  
- Vector store operativo para embeddings RAG
- Sistema de configuraci√≥n JSON hot-reload
- Registro centralizado de acciones MCP disponibles

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T1.1 | Crear estructura base Java Spring Boot con dependencias LangChain4j | ‚Äì | ‚úÖ |
| T1.2 | Implementar `LlmConfigurationService` para gesti√≥n de m√∫ltiples LLMs | T1.1 | ‚úÖ |
| T1.3 | Crear `VectorStoreService` para embeddings RAG (Chroma/In-memory) | T1.1, T1.2 | ‚úÖ |
| T1.4 | Dise√±ar `IntentConfigManager` para cargar intenciones desde JSON din√°mico | T1.1, T1.2 | ‚è≥ |
| T1.5 | Implementar `McpActionRegistry` para acciones configurables | T1.1, T1.2 | ‚è≥ |

---

## üìã **IMPLEMENTACI√ìN REAL COMPLETADA**

### **T1.1 ‚úÖ - Estructura Base Java Spring Boot**
**Archivos Implementados:**
- ‚úÖ `pom.xml` - Dependencias Spring Boot 3.2.1 + Spring Cloud + OpenAPI
- ‚úÖ `IntentManagerApplication.java` - Clase principal Spring Boot
- ‚úÖ `application.yml` - Configuraci√≥n simplificada para configms
- ‚úÖ `Dockerfile` - Multi-stage build optimizado
- ‚úÖ `configms/intent-manager.yml` - Configuraci√≥n centralizada

**Configuraci√≥n Centralizada:**
```yaml
# configms/src/main/resources/configurations/intent-manager.yml
llm:
  primary:
    model: ${PRIMARY_LLM_MODEL:gpt-4}
    provider: ${PRIMARY_LLM_PROVIDER:openai}
    api-key: ${OPENAI_API_KEY:}
  timeout: ${LLM_TIMEOUT:30s}
  max-tokens: ${LLM_MAX_TOKENS:4096}
  temperature: ${LLM_TEMPERATURE:0.7}
```

### **T1.2 ‚úÖ - LlmConfigurationService**
**Modelos de Dominio:**
- ‚úÖ `LlmProvider` - Enum (OPENAI, ANTHROPIC, GOOGLE, AZURE, LOCAL)
- ‚úÖ `LlmConfiguration` - Configuraci√≥n completa con timeout, tokens, temperatura
- ‚úÖ `LlmResponse` - Respuestas con metadatos y m√©tricas

**Servicios Implementados:**
- ‚úÖ `LlmConfigurationService` - Gesti√≥n centralizada de m√∫ltiples LLMs
- ‚úÖ `LlmInitializationService` - Inicializaci√≥n autom√°tica con `@EventListener`
- ‚úÖ `LlmConfigurationController` - 12 endpoints REST completos

**LLMs Configurados Autom√°ticamente:**
```
‚úÖ primary: LLM Primario (gpt-4) - Peso: 1.0
‚úÖ fallback: LLM de Fallback (gpt-3.5-turbo) - Peso: 0.8  
‚úÖ moe-llm-a: Juez A - An√°lisis Cr√≠tico (gpt-4) - Peso: 1.0
‚úÖ moe-llm-c: Juez C - Practicidad de Acci√≥n (gpt-3.5-turbo) - Peso: 0.9
‚ö†Ô∏è moe-llm-b: Omitido (API Key Anthropic no configurada)
```

**API REST Disponible:**
```bash
GET /api/v1/llm-config/statistics    # Estad√≠sticas completas
GET /api/v1/llm-config/primary      # LLM primario
GET /api/v1/llm-config/voting       # LLMs para votaci√≥n MoE
GET /api/v1/llm-config/{id}/health  # Health check individual
POST/PUT/DELETE /api/v1/llm-config  # CRUD completo
```

**Variables de Entorno Clave:**
```bash
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
MOE_ENABLED=true
PRIMARY_LLM_MODEL=gpt-4
MOE_LLM_A_MODEL=gpt-4
MOE_LLM_B_MODEL=claude-3-sonnet-20240229
MOE_LLM_C_MODEL=gpt-3.5-turbo
VECTOR_STORE_TYPE=in-memory
VECTOR_STORE_SIMILARITY_THRESHOLD=0.7
VECTOR_STORE_INIT_EXAMPLES=true
```

### **T1.3 ‚úÖ - VectorStoreService**
**Modelos de Dominio:**
- ‚úÖ `EmbeddingDocument` - Documentos con embeddings y metadata
- ‚úÖ `VectorStoreType` - Enum (IN_MEMORY, CHROMA, PINECONE, WEAVIATE, QDRANT)
- ‚úÖ `SearchResult` - Resultados de b√∫squeda con m√©tricas de similitud

**Servicios Implementados:**
- ‚úÖ `VectorStoreService` - Gesti√≥n completa de vector store
  - Soporte para In-memory y Chroma (preparado)
  - B√∫squeda por similitud coseno
  - CRUD completo de documentos
  - Estad√≠sticas y health checks
- ‚úÖ `VectorStoreInitializationService` - Inicializaci√≥n autom√°tica
  - Carga de ejemplos de prueba autom√°tica
  - Generaci√≥n de embeddings simulados
  - Verificaci√≥n de funcionalidad

**API REST Disponible:**
```bash
GET /api/v1/vector-store/statistics    # Estad√≠sticas completas
GET /api/v1/vector-store/health       # Health check
POST /api/v1/vector-store/documents   # A√±adir documento
GET /api/v1/vector-store/documents/{id} # Obtener documento
DELETE /api/v1/vector-store/documents/{id} # Eliminar documento
POST /api/v1/vector-store/search      # B√∫squeda por embedding
POST /api/v1/vector-store/search/text # B√∫squeda por texto
```

**Vector Store Configurado:**
```
‚úÖ Tipo: in-memory
‚úÖ Documentos cargados: 5 ejemplos de intenciones
‚úÖ Dimensi√≥n embedding: 1536 (OpenAI text-embedding-ada-002)
‚úÖ Umbral similitud: 0.7
‚úÖ B√∫squeda funcional: Similitud coseno implementada
‚úÖ Health check: ‚úÖ SALUDABLE
```

**B√∫squeda de Prueba Exitosa:**
```
Consulta: "tiempo"
Resultados: 3 documentos encontrados
Mejor coincidencia: "¬øqu√© tiempo hace hoy?" (similitud: 0.91)
Tiempo de b√∫squeda: < 10ms
```

---

## Epic 2 ‚Äì Motor RAG para Clasificaci√≥n de Intenciones

**Descripci√≥n del Epic**: Implementar el motor de Retrieval Augmented Generation (RAG) que reemplaza completamente el sistema RASA/DU. Utiliza embeddings vectoriales para realizar few-shot learning con ejemplos de intenciones almacenados din√°micamente. El sistema busca ejemplos similares, construye prompts contextuales y classifica intenciones sin necesidad de entrenamiento tradicional.

**Objetivos clave**:
- Clasificaci√≥n de intenciones sin entrenamiento previo
- Few-shot learning basado en ejemplos JSON
- Similarity search eficiente con embeddings
- Confidence scoring robusto y configurable
- Fallback inteligente para casos edge

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T2.1 | Crear `RagIntentClassifier` con embeddings vectoriales para few-shot learning | T1.3, T1.4 | ‚è≥ |
| T2.2 | Implementar sistema de similarity search para ejemplos de intenciones | T2.1 | ‚è≥ |
| T2.3 | Desarrollar prompt engineering din√°mico con contexto RAG | T2.1 | ‚è≥ |
| T2.4 | A√±adir confidence scoring usando m√∫ltiples m√©tricas | T2.2 | ‚è≥ |
| T2.5 | Crear fallback inteligente con degradaci√≥n gradual | T2.4 | ‚è≥ |

## Epic 3 ‚Äì MoE Voting System (Sistema de Votaci√≥n LLM)

**Descripci√≥n del Epic**: Implementar sistema de votaci√≥n donde m√∫ltiples LLMs debaten brevemente la mejor acci√≥n a tomar, reemplazando el concepto tradicional de "expertos especializados" por un "jurado de LLMs". Cada LLM vota independientemente y un motor de consenso determina la acci√≥n final. Sistema completamente configurable que puede habilitarse/deshabilitarse via variables de entorno.

**Objetivos clave**:
- Votaci√≥n simult√°nea de 3 LLMs con roles espec√≠ficos
- Motor de consenso para procesar votos y decidir acci√≥n final
- Configuraci√≥n flexible: habilitar/deshabilitar via MOE_ENABLED
- Fallback a LLM √∫nico cuando voting est√° deshabilitado
- Logging transparente del proceso de votaci√≥n para debugging

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T3.1 | Implementar `LlmVotingService` para sistema de debate entre m√∫ltiples LLMs | T1.2 | ‚è≥ |
| T3.2 | Crear `VotingRound` donde 3 LLMs debaten brevemente la acci√≥n a tomar | T3.1 | ‚è≥ |
| T3.3 | Desarrollar `ConsensusEngine` para procesar votos y llegar a decisi√≥n final | T3.2 | ‚è≥ |
| T3.4 | Implementar configuraci√≥n para habilitar/deshabilitar voting (MoE_ENABLED=true/false) | T3.3 | ‚è≥ |
| T3.5 | Crear fallback a LLM √∫nico cuando voting est√° deshabilitado | T3.4 | ‚è≥ |

## Epic 4 ‚Äì Sistema Conversacional Inteligente + Orquestaci√≥n de Subtareas

**Descripci√≥n del Epic**: Desarrollar sistema conversacional avanzado que usa LLM para descomponer din√°micamente peticiones complejas en m√∫ltiples subtareas ejecutables. NO usa configuraciones predefinidas, sino que el LLM analiza cada petici√≥n y identifica autom√°ticamente qu√© MCPs/servicios necesita invocar. Mantiene estado de progreso y marca conversaci√≥n como completada solo cuando todas las subtareas est√°n ejecutadas exitosamente.

**Objetivos clave**:
- Conversaci√≥n multivuelta con memoria contextual persistente
- Slot-filling autom√°tico usando LLM para preguntas din√°micas  
- **Descomposici√≥n din√°mica**: LLM identifica autom√°ticamente m√∫ltiples acciones en una petici√≥n
- **Orquestador inteligente**: Ejecuta subtareas secuencial o paralelamente seg√∫n dependencias detectadas
- **Estado de progreso**: Tracking autom√°tico hasta completion de todas las subtareas
- Manejo de an√°foras y referencias contextuales

**Casos de uso - Descomposici√≥n Din√°mica**:
- "Consulta el tiempo de Madrid y programa una alarma si va a llover" 
  ‚Üí LLM detecta: [consultar_tiempo + programar_alarma_condicional]
- "Crea un issue en GitHub sobre el weather bug y actualiza el estado en Taiga"
  ‚Üí LLM detecta: [crear_github_issue + actualizar_taiga_story]  
- "Enciende las luces del sal√≥n, pon m√∫sica relajante y ajusta la temperatura a 22¬∞"
  ‚Üí LLM detecta: [encender_luces + reproducir_musica + ajustar_temperatura]

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T4.1 | Dise√±ar `ConversationManager` con contexto LLM-powered | T2.1 | ‚è≥ |
| T4.2 | Implementar slot-filling autom√°tico usando LLM para preguntas din√°micas | T4.1 | ‚è≥ |
| T4.3 | Crear `EntityExtractor` basado en LLM para extracci√≥n contextual | T4.1 | ‚è≥ |
| T4.4 | Desarrollar memoria conversacional con Redis para sesiones persistentes | T4.1 | ‚è≥ |
| T4.5 | Crear `DynamicSubtaskDecomposer` - LLM analiza petici√≥n y identifica m√∫ltiples acciones autom√°ticamente | T4.1 | ‚è≥ |
| T4.6 | Implementar `TaskOrchestrator` para ejecuci√≥n secuencial/paralela de subtareas detectadas din√°micamente | T4.5 | ‚è≥ |
| T4.7 | Desarrollar sistema de estado de progreso: tracking autom√°tico hasta completion de todas las subtareas | T4.6 | ‚è≥ |
| T4.8 | Implementar resoluci√≥n de an√°foras y referencias contextuales | T4.4 | ‚è≥ |

## Epic 5 ‚Äì Integraci√≥n Audio y Transcripci√≥n

**Descripci√≥n del Epic**: Integrar completamente el pipeline de audio desde recepci√≥n hasta respuesta. Reemplaza la dependencia de servicios externos de transcripci√≥n por integraci√≥n directa con whisper-ms, maneja metadata de audio contextual (ubicaci√≥n, temperatura, etc.), y soporta tanto entrada de texto como audio de forma unificada.

**Objetivos clave**:
- Pipeline completo: Audio ‚Üí Transcripci√≥n ‚Üí Clasificaci√≥n ‚Üí Respuesta
- Integraci√≥n directa con whisper-ms para transcripci√≥n
- Soporte para metadata contextual de dispositivos (temperatura, ubicaci√≥n)
- Manejo robusto de errores de transcripci√≥n
- API unificada para texto y audio

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T5.1 | Crear `AudioProcessingController` para recibir audio multipart/form-data | T1.1 | ‚è≥ |
| T5.2 | Implementar cliente `WhisperTranscriptionService` para whisper-ms | T5.1 | ‚è≥ |
| T5.3 | Desarrollar pipeline Audio ‚Üí Transcripci√≥n ‚Üí Clasificaci√≥n ‚Üí Respuesta | T5.2, T2.1 | ‚è≥ |
| T5.4 | A√±adir soporte para metadata de audio y contexto de dispositivo | T5.1 | ‚è≥ |
| T5.5 | Implementar manejo de errores de transcripci√≥n con fallbacks | T5.2 | ‚è≥ |

## Epic 6 ‚Äì MCP (Model Context Protocol) Integration

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T6.1 | Crear `McpClientService` para comunicaci√≥n con servicios MCP externos | T1.5 | ‚è≥ |
| T6.2 | Implementar `TaigaMcpClient` reutilizando taiga-mcp-ms existente | T6.1 | ‚è≥ |
| T6.3 | Desarrollar `WeatherMcpClient` para consultas meteorol√≥gicas | T6.1 | ‚è≥ |
| T6.4 | Crear `SystemMcpClient` para acciones de sistema (hora, alarmas, etc.) | T6.1 | ‚è≥ |
| T6.5 | A√±adir configuraci√≥n JSON din√°mica para nuevos clientes MCP | T6.1 | ‚è≥ |

## Epic 7 ‚Äì API y Compatibilidad

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T7.1 | Dise√±ar nuevos endpoints REST manteniendo compatibilidad con gatewayms | T5.1 | ‚è≥ |
| T7.2 | Crear `AssistantController` con soporte para audio + texto + contexto | T7.1, T5.3 | ‚è≥ |
| T7.3 | Implementar respuestas m√∫ltiples: texto, audio TTS, acciones MCP | T7.2 | ‚è≥ |
| T7.4 | A√±adir endpoints de diagn√≥stico y m√©tricas de rendimiento | T7.2 | ‚è≥ |
| T7.5 | Crear documentaci√≥n OpenAPI/Swagger para nuevos endpoints | T7.1 | ‚è≥ |

## Epic 8 ‚Äì Configuraci√≥n JSON Din√°mica

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T8.1 | Dise√±ar esquema JSON para definici√≥n de intenciones y ejemplos | T1.4 | ‚è≥ |
| T8.2 | Crear esquema JSON para configuraci√≥n de expertos MoE | T3.1 | ‚è≥ |
| T8.3 | Implementar esquema JSON para acciones MCP y par√°metros | T1.5 | ‚è≥ |
| T8.4 | A√±adir hot-reload de configuraciones sin reiniciar el servicio | T8.1, T8.2, T8.3 | ‚è≥ |
| T8.5 | Crear validaci√≥n de esquemas JSON con feedback detallado | T8.4 | ‚è≥ |

## Epic 9 ‚Äì Testing y Evaluaci√≥n

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T9.1 | Crear suite de tests unitarios para todos los componentes | Todas | ‚è≥ |
| T9.2 | Implementar tests de integraci√≥n para flujo completo audio‚Üírespuesta | T5.3, T7.3 | ‚è≥ |
| T9.3 | Desarrollar benchmarks de rendimiento vs sistema RASA/DU anterior | T2.1, T3.5 | ‚è≥ |
| T9.4 | Crear tests de escalabilidad para adici√≥n din√°mica de intenciones | T8.4 | ‚è≥ |
| T9.5 | Implementar m√©tricas de calidad conversacional y accuracy | T4.2, T4.5 | ‚è≥ |

## Epic 10 ‚Äì Despliegue y Configuraci√≥n

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T10.1 | Actualizar Dockerfile para nuevas dependencias LangChain4j | T1.1 | ‚è≥ |
| T10.2 | Configurar variables de entorno para LLMs y servicios externos | T1.2 | ‚è≥ |
| T10.3 | Crear archivos JSON de configuraci√≥n por defecto para demo | T8.1, T8.2, T8.3 | ‚è≥ |
| T10.4 | Integrar con docker-compose existente manteniendo compatibilidad | T10.1 | ‚è≥ |
| T10.5 | Documentar migraci√≥n desde sistema anterior | Todas | ‚è≥ |

---

## Roadmap de Implementaci√≥n

### ‚úÖ **Fase 1: Fundamentos (COMPLETADA)**
- ‚úÖ **Epic 1**: Arquitectura Base (T1.1, T1.2 completados)
- ‚è≥ **Epic 1**: Resto de tareas (T1.3, T1.4, T1.5)
- ‚è≥ **Epic 2**: Motor RAG b√°sico
- ‚è≥ **Epic 5**: Integraci√≥n Audio (b√°sica)

### üîÑ **Fase 2: Inteligencia (EN PROGRESO)**
- ‚úÖ **Epic 3**: MoE Architecture (Base T1.2 completada)
- ‚è≥ **Epic 4**: Sistema Conversacional
- ‚è≥ **Epic 8**: Configuraci√≥n JSON

### ‚è≥ **Fase 3: Integraci√≥n**
- ‚è≥ **Epic 6**: MCP Integration
- ‚è≥ **Epic 7**: API y Compatibilidad
- ‚è≥ **Epic 9**: Testing b√°sico

### ‚è≥ **Fase 4: Optimizaci√≥n**
- ‚è≥ **Epic 9**: Testing completo y evaluaci√≥n
- ‚è≥ **Epic 10**: Despliegue y documentaci√≥n

### **üìä Progreso Actual:**
- **Epic 1**: 3/5 tareas completadas (60%)
- **Epic 2**: 0/5 tareas completadas (0%)
- **Epic 3**: Base preparada, pendiente implementaci√≥n completa
- **Total General**: 3/50 tareas completadas (6%)

---

## Arquitectura Implementada vs Objetivo

### **‚úÖ IMPLEMENTADO (T1.1 + T1.2)**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    Config    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    REST API    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CONFIGMS      ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  INTENTMGR-V2   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  LLM CONFIG     ‚îÇ
‚îÇ (Centralizada)  ‚îÇ              ‚îÇ (Spring Boot)   ‚îÇ                ‚îÇ   SERVICE       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                          ‚îÇ                                ‚îÇ
                                          ‚ñº                                ‚ñº
                                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                 ‚îÇ LLM INIT SERVICE‚îÇ              ‚îÇ LLM CONFIG API  ‚îÇ
                                 ‚îÇ (Auto Setup)    ‚îÇ              ‚îÇ (12 endpoints)  ‚îÇ
                                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                          ‚îÇ                                ‚îÇ
                                          ‚ñº                                ‚ñº
                                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                 ‚îÇ   LLM POOL      ‚îÇ              ‚îÇ   LLM HEALTH    ‚îÇ
                                 ‚îÇ (4 LLMs Ready)  ‚îÇ              ‚îÇ   (Monitoring)  ‚îÇ
                                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                          ‚îÇ
                                          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PRIMARY LLM   ‚îÇ              ‚îÇ   MOE LLM-A     ‚îÇ    ‚îÇ   MOE LLM-C     ‚îÇ
‚îÇ   (GPT-4)       ‚îÇ              ‚îÇ (GPT-4 Juez A)  ‚îÇ    ‚îÇ(GPT-3.5 Juez C) ‚îÇ
‚îÇ   Peso: 1.0     ‚îÇ              ‚îÇ   Peso: 1.0     ‚îÇ    ‚îÇ   Peso: 0.9     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                                ‚îÇ                        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚ñº
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ FALLBACK LLM    ‚îÇ
                ‚îÇ (GPT-3.5-Turbo) ‚îÇ
                ‚îÇ   Peso: 0.8     ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **üéØ OBJETIVO COMPLETO (Futuras Tareas)**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    Audio     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    JSON    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  WHISPER-MS     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  GATEWAY-MS     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ INTENTMGR-V2    ‚îÇ
‚îÇ  (Transcripci√≥n)‚îÇ              ‚îÇ  (Routing)      ‚îÇ            ‚îÇ (LLM-RAG+MoE)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                          ‚îÇ
                                                                          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ VECTOR STORE    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   RAG ENGINE    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  EXPERT ROUTER  ‚îÇ
‚îÇ (Embeddings)    ‚îÇ              ‚îÇ (Similarity)    ‚îÇ            ‚îÇ  (MoE Selection) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                          ‚îÇ
                                                                          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     LLM-A       ‚îÇ              ‚îÇ     LLM-B       ‚îÇ    ‚îÇ     LLM-C       ‚îÇ
‚îÇ   (GPT-4)       ‚îÇ              ‚îÇ  (Claude-3)     ‚îÇ    ‚îÇ (GPT-3.5-Turbo) ‚îÇ
‚îÇ   "Voto: X"     ‚îÇ              ‚îÇ   "Voto: Y"     ‚îÇ    ‚îÇ   "Voto: Z"     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                                ‚îÇ                        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚ñº
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ CONSENSUS ENGINE‚îÇ
                ‚îÇ (Procesa votos) ‚îÇ
                ‚îÇ Decisi√≥n final  ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CONVERSATION    ‚îÇ              ‚îÇ   MCP ACTIONS   ‚îÇ    ‚îÇ   SINGLE LLM    ‚îÇ
‚îÇ   MANAGER       ‚îÇ              ‚îÇ (Taiga/Weather) ‚îÇ    ‚îÇ   (Fallback)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Configuraci√≥n de Ejemplo

### intents.json
```json
{
  "intents": {
    "consultar_tiempo": {
      "examples": [
        "¬øqu√© tiempo hace?",
        "dime el clima de hoy",
        "c√≥mo est√° el tiempo"
      ],
      "required_entities": ["ubicacion"],
      "mcp_action": "weather_query",
      "expert_domain": "general",
      "slot_filling_questions": {
        "ubicacion": "¬øDe qu√© ciudad quieres consultar el tiempo?"
      }
    },
    "encender_luz": {
      "examples": [
        "enciende la luz",
        "prende la l√°mpara",
        "ilumina la habitaci√≥n"
      ],
      "required_entities": ["lugar"],
      "mcp_action": "smart_home_light_on",
      "expert_domain": "smart_home",
      "slot_filling_questions": {
        "lugar": "¬øEn qu√© habitaci√≥n quieres encender la luz?"
      }
    }
  }
}
```

### moe_voting.json (Implementado)
```json
{
  "voting_system": {
    "enabled": "${MOE_ENABLED:true}",
    "max_debate_rounds": 1,
    "consensus_threshold": 0.6,
    "llm_participants": [
      {
        "id": "moe-llm-a", 
        "model": "gpt-4",
        "api_key": "${OPENAI_API_KEY}",
        "role": "Juez A - An√°lisis cr√≠tico y evaluaci√≥n detallada",
        "weight": 1.0,
        "temperature": 0.3,
        "max_tokens": 2048
      },
      {
        "id": "moe-llm-b",
        "model": "claude-3-sonnet-20240229", 
        "api_key": "${ANTHROPIC_API_KEY}",
        "role": "Juez B - An√°lisis de contexto conversacional y coherencia",
        "weight": 1.0,
        "temperature": 0.3,
        "max_tokens": 2048
      },
      {
        "id": "moe-llm-c",
        "model": "gpt-3.5-turbo",
        "api_key": "${OPENAI_API_KEY}",
        "role": "Juez C - Evaluaci√≥n de practicidad y factibilidad de acciones",
        "weight": 0.9,
        "temperature": 0.3,
        "max_tokens": 2048
      }
    ],
    "fallback_llm": {
      "id": "primary",
      "model": "gpt-4",
      "api_key": "${OPENAI_API_KEY}",
      "used_when": "voting_disabled_or_failed"
    }
  }
}
```

### Ejemplo de Flujo de Votaci√≥n Simple
```
Usuario: "Enciende la luz del sal√≥n"

LLM-A Voto: "ACCI√ìN: encender_luz, ENTIDADES: {lugar: 'sal√≥n'}, CONFIANZA: 0.9"
LLM-B Voto: "ACCI√ìN: encender_luz, ENTIDADES: {lugar: 'sal√≥n'}, CONFIANZA: 0.85" 
LLM-C Voto: "ACCI√ìN: encender_luz, ENTIDADES: {lugar: 'sal√≥n'}, CONFIANZA: 0.8"

CONSENSO: encender_luz (3/3 votos) ‚Üí EJECUTAR ACCI√ìN MCP
```

### Ejemplo de Descomposici√≥n Din√°mica por LLM
```
Usuario: "Consulta el tiempo de Madrid y programa una alarma si va a llover"

PASO 1 - AN√ÅLISIS LLM:
LLM Prompt: "Analiza esta petici√≥n y lista las acciones MCP necesarias: 'Consulta el tiempo de Madrid y programa una alarma si va a llover'"

LLM Response: {
  "subtasks": [
    {
      "action": "consultar_tiempo",
      "entities": {"ubicacion": "Madrid"},
      "description": "Consultar informaci√≥n meteorol√≥gica de Madrid"
    },
    {
      "action": "programar_alarma_condicional", 
      "entities": {"condicion": "si_llueve", "ubicacion": "Madrid"},
      "description": "Programar alarma basada en condici√≥n meteorol√≥gica",
      "depends_on_result": "consultar_tiempo"
    }
  ]
}

PASO 2 - EJECUCI√ìN ORQUESTADA:
- Estado inicial: PENDING [consultar_tiempo, programar_alarma_condicional]
- Ejecuta: consultar_tiempo(Madrid) ‚Üí Resultado: "Lluvia probable 70%"  
- Estado: PENDING [programar_alarma_condicional], COMPLETED [consultar_tiempo]
- Ejecuta: programar_alarma_condicional(condicion="si_llueve") ‚Üí Resultado: "Alarma creada"
- Estado: COMPLETED [consultar_tiempo, programar_alarma_condicional]
- CONVERSACI√ìN COMPLETADA: "En Madrid hay 70% probabilidad de lluvia. He programado una alarma para avisarte."
```

### Configuraci√≥n de MCPs Disponibles (mcp_registry.json)
```json
{
  "available_mcps": {
    "consultar_tiempo": {
      "description": "Consulta informaci√≥n meteorol√≥gica de una ubicaci√≥n",
      "required_entities": ["ubicacion"],
      "service_endpoint": "weather-mcp-service"
    },
    "programar_alarma": {
      "description": "Programa una alarma para una fecha/hora espec√≠fica", 
      "required_entities": ["fecha_hora", "mensaje"],
      "service_endpoint": "system-mcp-service"
    },
    "programar_alarma_condicional": {
      "description": "Programa alarma basada en condiciones externas",
      "required_entities": ["condicion"],
      "service_endpoint": "system-mcp-service"
    },
    "crear_github_issue": {
      "description": "Crea un issue en un repositorio de GitHub",
      "required_entities": ["titulo", "descripcion"],
      "service_endpoint": "github-mcp-service"
    }
  }
}
```