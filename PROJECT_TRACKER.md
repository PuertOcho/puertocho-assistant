# PROJECT TRACKER ‚Äì IntentManagerMS LLM-RAG + MoE Implementation

> √öltima actualizaci√≥n: 2025-01-27 (Reorganizado con documentaci√≥n en archivos espec√≠ficos)
> 
> **Objetivo**: Implementaci√≥n completamente nueva de intentmanagerms usando arquitectura LLM-RAG + Mixture of Experts (MoE) para clasificaci√≥n de intenciones escalable, conversaci√≥n multivuelta inteligente y soporte nativo MCP.

---

## Leyenda de estados
- ‚úÖ Completado
- üîÑ En progreso
- ‚è≥ Pendiente
- üöß Bloqueado

---

## Epic 1 ‚Äì Arquitectura Base y Configuraci√≥n LLM-RAG

**Descripci√≥n del Epic**: Establecer los fundamentos t√©cnicos del sistema LLM-RAG. Este Epic crea la infraestructura base necesaria para soportar m√∫ltiples LLMs, almacenamiento vectorial para RAG, configuraci√≥n din√°mica desde JSON, y el registro de acciones MCP. Es la base sobre la que se construyen todos los dem√°s Epics.

**üìã Documentaci√≥n Completa**: [Epic1.md](puertocho-assistant-server/intentmanagerms/iadocs/Epic1.md)

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T1.1 | Crear estructura base Java Spring Boot con dependencias LangChain4j | ‚Äì | ‚úÖ |
| T1.2 | Implementar `LlmConfigurationService` para gesti√≥n de m√∫ltiples LLMs | T1.1 | ‚úÖ |
| T1.3 | Crear `VectorStoreService` para embeddings RAG (Chroma/In-memory) | T1.1, T1.2 | ‚úÖ |
| T1.4 | Dise√±ar `IntentConfigManager` para cargar intenciones desde JSON din√°mico | T1.1, T1.2 | ‚úÖ |
| T1.5 | Implementar `McpActionRegistry` para acciones configurables | T1.1, T1.2 | ‚úÖ |

**üéâ Epic 1 Completado al 100%**
- ‚úÖ Infraestructura Base: OPERATIVA
- ‚úÖ Configuraci√≥n Din√°mica: FUNCIONANDO
- ‚úÖ API REST: 67 endpoints operativos
- ‚úÖ Pruebas: 100% exitosas

---

## Epic 2 ‚Äì Motor RAG para Clasificaci√≥n de Intenciones

**Descripci√≥n del Epic**: Implementar el motor de Retrieval Augmented Generation (RAG) que reemplaza completamente el sistema RASA/DU. Utiliza embeddings vectoriales para realizar few-shot learning con ejemplos de intenciones almacenados din√°micamente. El sistema busca ejemplos similares, construye prompts contextuales y classifica intenciones sin necesidad de entrenamiento tradicional.

**üìã Documentaci√≥n Completa**: [Epic2.md](puertocho-assistant-server/intentmanagerms/iadocs/Epic2.md)

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T2.1 | Crear `RagIntentClassifier` con embeddings vectoriales para few-shot learning | T1.3, T1.4 | ‚úÖ |
| T2.2 | Implementar sistema de similarity search para ejemplos de intenciones | T2.1 | ‚úÖ |
| T2.3 | Desarrollar prompt engineering din√°mico con contexto RAG | T2.1 | ‚úÖ |
| T2.4 | A√±adir confidence scoring usando m√∫ltiples m√©tricas | T2.2 | ‚úÖ |
| T2.5 | Crear fallback inteligente con degradaci√≥n gradual | T2.4 | ‚úÖ |

**üéâ Epic 2 Completado al 100%**
- ‚úÖ Motor RAG: OPERATIVO
- ‚úÖ Clasificaci√≥n de Intenciones: FUNCIONANDO
- ‚úÖ Confidence Scoring: 10 m√©tricas implementadas
- ‚úÖ Fallback Inteligente: 5 niveles de degradaci√≥n
- ‚úÖ API REST: 26 endpoints operativos

---

## Epic 3 ‚Äì MoE Voting System (Sistema de Votaci√≥n LLM)

**Descripci√≥n del Epic**: Implementar sistema de votaci√≥n donde m√∫ltiples LLMs debaten brevemente la mejor acci√≥n a tomar, reemplazando el concepto tradicional de "expertos especializados" por un "jurado de LLMs". Cada LLM vota independientemente y un motor de consenso determina la acci√≥n final. Sistema completamente configurable que puede habilitarse/deshabilitarse via variables de entorno.

**üìã Documentaci√≥n Completa**: [Epic3.md](puertocho-assistant-server/intentmanagerms/iadocs/Epic3.md)

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T3.1 | Implementar `LlmVotingService` para sistema de debate entre m√∫ltiples LLMs | T1.2 | ‚úÖ |
| T3.2 | Crear `VotingRound` donde 3 LLMs debaten brevemente la acci√≥n a tomar | T3.1 | ‚úÖ |
| T3.3 | Desarrollar `ConsensusEngine` para procesar votos y llegar a decisi√≥n final | T3.2 | ‚úÖ |
| T3.4 | Implementar configuraci√≥n para habilitar/deshabilitar voting (MoE_ENABLED=true/false) | T3.3 | ‚úÖ |
| T3.5 | Crear fallback a LLM √∫nico cuando voting est√° deshabilitado | T3.4 | ‚úÖ |

**üéâ Epic 3 Completado al 100%**
- ‚úÖ Sistema de Votaci√≥n: OPERATIVO
- ‚úÖ Motor de Consenso: FUNCIONANDO
- ‚úÖ Fallback Autom√°tico: ACTIVO
- ‚úÖ Configuraci√≥n Din√°mica: HABILITADA
- ‚úÖ API REST: 10 endpoints operativos

---

## Epic 4 ‚Äì Sistema Conversacional Inteligente + Orquestaci√≥n de Subtareas

**Descripci√≥n del Epic**: Desarrollar sistema conversacional avanzado que usa LLM para descomponer din√°micamente peticiones complejas en m√∫ltiples subtareas ejecutables. NO usa configuraciones predefinidas, sino que el LLM analiza cada petici√≥n y identifica autom√°ticamente qu√© MCPs/servicios necesita invocar. Mantiene estado de progreso y marca conversaci√≥n como completada solo cuando todas las subtareas est√°n ejecutadas exitosamente.

**Objetivos clave**:
- Conversaci√≥n multivuelta con memoria contextual persistente
- Slot-filling autom√°tico usando LLM para preguntas din√°micas  
- **Descomposici√≥n din√°mica**: LLM identifica autom√°ticamente m√∫ltiples acciones en una petici√≥n
- **Orquestador inteligente**: Ejecuta subtareas secuencial o paralelamente seg√∫n dependencias detectadas
- **Estado de progreso**: Tracking autom√°tico hasta completion de todas las subtareas
- Manejo de an√°foras y referencias contextuales

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T4.1 | Dise√±ar `ConversationManager` con contexto LLM-powered | T2.1 | ‚úÖ |
| T4.2 | Implementar slot-filling autom√°tico usando LLM para preguntas din√°micas | T4.1 | ‚úÖ |
| T4.3 | Crear `EntityExtractor` basado en LLM para extracci√≥n contextual | T4.1 | ‚úÖ |
| T4.4 | Desarrollar memoria conversacional con Redis para sesiones persistentes | T4.1 | ‚úÖ |
| T4.5 | Crear `DynamicSubtaskDecomposer` - LLM analiza petici√≥n y identifica m√∫ltiples acciones autom√°ticamente | T4.1 | ‚úÖ |
| T4.6 | Implementar `TaskOrchestrator` para ejecuci√≥n secuencial/paralela de subtareas detectadas din√°micamente | T4.5 | ‚è≥ |
| T4.7 | Desarrollar sistema de estado de progreso: tracking autom√°tico hasta completion de todas las subtareas | T4.6 | ‚è≥ |
| T4.8 | Implementar resoluci√≥n de an√°foras y referencias contextuales | T4.4 | ‚è≥ |

---

## Epic 5 ‚Äì Integraci√≥n Audio y Transcripci√≥n

**Descripci√≥n del Epic**: Integrar completamente el pipeline de audio desde recepci√≥n hasta respuesta. Reemplaza la dependencia de servicios externos de transcripci√≥n por integraci√≥n directa con whisper-ms, maneja metadata de audio contextual (ubicaci√≥n, temperatura, etc.), y soporta tanto entrada de texto como audio de forma unificada.

**Objetivos clave**:
- Pipeline completo: Audio ‚Üí Transcripci√≥n ‚Üí Clasificaci√≥n ‚Üí Respuesta
- Integraci√≥n directa con whisper-ms para transcripci√≥n
- Soporte para metadata contextual de dispositivos (temperatura, ubicaci√≥n)
- Manejo robusto de errores de transcripci√≥n
- API unificada para texto y audio

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T5.1 | Crear `AudioProcessingController` para recibir audio multipart/form-data | T1.1 | ‚è≥ |
| T5.2 | Implementar cliente `WhisperTranscriptionService` para whisper-ms | T5.1 | ‚è≥ |
| T5.3 | Desarrollar pipeline Audio ‚Üí Transcripci√≥n ‚Üí Clasificaci√≥n ‚Üí Respuesta | T5.2, T2.1 | ‚è≥ |
| T5.4 | A√±adir soporte para metadata de audio y contexto de dispositivo | T5.1 | ‚è≥ |
| T5.5 | Implementar manejo de errores de transcripci√≥n con fallbacks | T5.2 | ‚è≥ |

---

## Epic 6 ‚Äì MCP (Model Context Protocol) Integration

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T6.1 | Crear `McpClientService` para comunicaci√≥n con servicios MCP externos | T1.5 | ‚è≥ |
| T6.2 | Implementar `TaigaMcpClient` reutilizando taiga-mcp-ms existente | T6.1 | ‚è≥ |
| T6.3 | Desarrollar `WeatherMcpClient` para consultas meteorol√≥gicas | T6.1 | ‚è≥ |
| T6.4 | Crear `SystemMcpClient` para acciones de sistema (hora, alarmas, etc.) | T6.1 | ‚è≥ |
| T6.5 | A√±adir configuraci√≥n JSON din√°mica para nuevos clientes MCP | T6.1 | ‚è≥ |

---

## Epic 7 ‚Äì API y Compatibilidad

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T7.1 | Dise√±ar nuevos endpoints REST manteniendo compatibilidad con gatewayms | T5.1 | ‚è≥ |
| T7.2 | Crear `AssistantController` con soporte para audio + texto + contexto | T7.1, T5.3 | ‚è≥ |
| T7.3 | Implementar respuestas m√∫ltiples: texto, audio TTS, acciones MCP | T7.2 | ‚è≥ |
| T7.4 | A√±adir endpoints de diagn√≥stico y m√©tricas de rendimiento | T7.2 | ‚è≥ |
| T7.5 | Crear documentaci√≥n OpenAPI/Swagger para nuevos endpoints | T7.1 | ‚è≥ |

---

## Epic 8 ‚Äì Configuraci√≥n JSON Din√°mica

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T8.1 | Dise√±ar esquema JSON para definici√≥n de intenciones y ejemplos | T1.4 | ‚è≥ |
| T8.2 | Crear esquema JSON para configuraci√≥n de expertos MoE | T3.1 | ‚è≥ |
| T8.3 | Implementar esquema JSON para acciones MCP y par√°metros | T1.5 | ‚è≥ |
| T8.4 | A√±adir hot-reload de configuraciones sin reiniciar el servicio | T8.1, T8.2, T8.3 | ‚è≥ |
| T8.5 | Crear validaci√≥n de esquemas JSON con feedback detallado | T8.4 | ‚è≥ |

---

## Epic 9 ‚Äì Testing y Evaluaci√≥n

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T9.1 | Crear suite de tests unitarios para todos los componentes | Todas | ‚è≥ |
| T9.2 | Implementar tests de integraci√≥n para flujo completo audio‚Üírespuesta | T5.3, T7.3 | ‚è≥ |
| T9.3 | Desarrollar benchmarks de rendimiento vs sistema RASA/DU anterior | T2.1, T3.5 | ‚è≥ |
| T9.4 | Crear tests de escalabilidad para adici√≥n din√°mica de intenciones | T8.4 | ‚è≥ |
| T9.5 | Implementar m√©tricas de calidad conversacional y accuracy | T4.2, T4.5 | ‚è≥ |

---

## Epic 10 ‚Äì Despliegue y Configuraci√≥n

| ID | Descripci√≥n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T10.1 | Actualizar Dockerfile para nuevas dependencias LangChain4j | T1.1 | ‚è≥ |
| T10.2 | Configurar variables de entorno para LLMs y servicios externos | T1.2 | ‚è≥ |
| T10.3 | Crear archivos JSON de configuraci√≥n por defecto para demo | T8.1, T8.2, T8.3 | ‚è≥ |
| T10.4 | Integrar con docker-compose existente manteniendo compatibilidad | T10.1 | ‚è≥ |
| T10.5 | Documentar migraci√≥n desde sistema anterior | Todas | ‚è≥ |

---

## Roadmap de Implementaci√≥n

### ‚úÖ **Fase 1: Fundamentos (COMPLETADA)**
- ‚úÖ **Epic 1**: Arquitectura Base (T1.1, T1.2, T1.3, T1.4, T1.5 completados)
- ‚úÖ **Epic 2**: Motor RAG (T2.1, T2.2, T2.3, T2.4, T2.5 completados)
- ‚úÖ **Epic 3**: MoE Architecture (T3.1, T3.2, T3.3, T3.4, T3.5 completados)

### üîÑ **Fase 2: Inteligencia (EN PROGRESO)**
- ‚è≥ **Epic 4**: Sistema Conversacional
- ‚è≥ **Epic 8**: Configuraci√≥n JSON

### ‚è≥ **Fase 3: Integraci√≥n**
- ‚è≥ **Epic 5**: Integraci√≥n Audio
- ‚è≥ **Epic 6**: MCP Integration
- ‚è≥ **Epic 7**: API y Compatibilidad

### ‚è≥ **Fase 4: Optimizaci√≥n**
- ‚è≥ **Epic 9**: Testing completo y evaluaci√≥n
- ‚è≥ **Epic 10**: Despliegue y documentaci√≥n

### **üìä Progreso Actual:**
- **Epic 1**: 5/5 tareas completadas (100%) ‚úÖ
- **Epic 2**: 5/5 tareas completadas (100%) ‚úÖ
- **Epic 3**: 5/5 tareas completadas (100%) ‚úÖ
- **Epic 4**: 3/8 tareas completadas (37.5%) üîÑ
- **Total General**: 18/50 tareas completadas (36%)

---

## Casos de Uso - Descomposici√≥n Din√°mica

**Ejemplos de peticiones complejas que el sistema debe descomponer autom√°ticamente:**

1. **"Consulta el tiempo de Madrid y programa una alarma si va a llover"** 
   ‚Üí LLM detecta: [consultar_tiempo + programar_alarma_condicional]

2. **"Crea un issue en GitHub sobre el weather bug y actualiza el estado en Taiga"**
   ‚Üí LLM detecta: [crear_github_issue + actualizar_taiga_story]  

3. **"Enciende las luces del sal√≥n, pon m√∫sica relajante y ajusta la temperatura a 22¬∞"**
   ‚Üí LLM detecta: [encender_luces + reproducir_musica + ajustar_temperatura]

---

## Configuraci√≥n de Ejemplo

### intents.json
```json
{
  "intents": {
    "consultar_tiempo": {
      "examples": [
        "¬øqu√© tiempo hace?",
        "dime el clima de hoy",
        "c√≥mo est√° el tiempo"
      ],
      "required_entities": ["ubicacion"],
      "mcp_action": "weather_query",
      "expert_domain": "general",
      "slot_filling_questions": {
        "ubicacion": "¬øDe qu√© ciudad quieres consultar el tiempo?"
      }
    },
    "encender_luz": {
      "examples": [
        "enciende la luz",
        "prende la l√°mpara",
        "ilumina la habitaci√≥n"
      ],
      "required_entities": ["lugar"],
      "mcp_action": "smart_home_light_on",
      "expert_domain": "smart_home",
      "slot_filling_questions": {
        "lugar": "¬øEn qu√© habitaci√≥n quieres encender la luz?"
      }
    }
  }
}
```

### moe_voting.json
```json
{
  "voting_system": {
    "enabled": "${MOE_ENABLED:true}",
    "max_debate_rounds": 1,
    "consensus_threshold": 0.6,
    "llm_participants": [
      {
        "id": "moe-llm-a", 
        "model": "gpt-4",
        "api_key": "${OPENAI_API_KEY}",
        "role": "Juez A - An√°lisis cr√≠tico y evaluaci√≥n detallada",
        "weight": 1.0,
        "temperature": 0.3,
        "max_tokens": 2048
      },
      {
        "id": "moe-llm-b",
        "model": "claude-3-sonnet-20240229", 
        "api_key": "${ANTHROPIC_API_KEY}",
        "role": "Juez B - An√°lisis de contexto conversacional y coherencia",
        "weight": 1.0,
        "temperature": 0.3,
        "max_tokens": 2048
      },
      {
        "id": "moe-llm-c",
        "model": "gpt-3.5-turbo",
        "api_key": "${OPENAI_API_KEY}",
        "role": "Juez C - Evaluaci√≥n de practicidad y factibilidad de acciones",
        "weight": 0.9,
        "temperature": 0.3,
        "max_tokens": 2048
      }
    ],
    "fallback_llm": {
      "id": "primary",
      "model": "gpt-4",
      "api_key": "${OPENAI_API_KEY}",
      "used_when": "voting_disabled_or_failed"
    }
  }
}
```

### Ejemplo de Flujo de Votaci√≥n Simple
```
Usuario: "Enciende la luz del sal√≥n"

LLM-A Voto: "ACCI√ìN: encender_luz, ENTIDADES: {lugar: 'sal√≥n'}, CONFIANZA: 0.9"
LLM-B Voto: "ACCI√ìN: encender_luz, ENTIDADES: {lugar: 'sal√≥n'}, CONFIANZA: 0.85" 
LLM-C Voto: "ACCI√ìN: encender_luz, ENTIDADES: {lugar: 'sal√≥n'}, CONFIANZA: 0.8"

CONSENSO: encender_luz (3/3 votos) ‚Üí EJECUTAR ACCI√ìN MCP
```

### Ejemplo de Descomposici√≥n Din√°mica por LLM
```
Usuario: "Consulta el tiempo de Madrid y programa una alarma si va a llover"

PASO 1 - AN√ÅLISIS LLM:
LLM Prompt: "Analiza esta petici√≥n y lista las acciones MCP necesarias: 'Consulta el tiempo de Madrid y programa una alarma si va a llover'"

LLM Response: {
  "subtasks": [
    {
      "action": "consultar_tiempo",
      "entities": {"ubicacion": "Madrid"},
      "description": "Consultar informaci√≥n meteorol√≥gica de Madrid"
    },
    {
      "action": "programar_alarma_condicional", 
      "entities": {"condicion": "si_llueve", "ubicacion": "Madrid"},
      "description": "Programar alarma basada en condici√≥n meteorol√≥gica",
      "depends_on_result": "consultar_tiempo"
    }
  ]
}

PASO 2 - EJECUCI√ìN ORQUESTADA:
- Estado inicial: PENDING [consultar_tiempo, programar_alarma_condicional]
- Ejecuta: consultar_tiempo(Madrid) ‚Üí Resultado: "Lluvia probable 70%"  
- Estado: PENDING [programar_alarma_condicional], COMPLETED [consultar_tiempo]
- Ejecuta: programar_alarma_condicional(condicion="si_llueve") ‚Üí Resultado: "Alarma creada"
- Estado: COMPLETED [consultar_tiempo, programar_alarma_condicional]
- CONVERSACI√ìN COMPLETADA: "En Madrid hay 70% probabilidad de lluvia. He programado una alarma para avisarte."
```

### Configuraci√≥n de MCPs Disponibles (mcp_registry.json)
```json
{
  "available_mcps": {
    "consultar_tiempo": {
      "description": "Consulta informaci√≥n meteorol√≥gica de una ubicaci√≥n",
      "required_entities": ["ubicacion"],
      "service_endpoint": "weather-mcp-service"
    },
    "programar_alarma": {
      "description": "Programa una alarma para una fecha/hora espec√≠fica", 
      "required_entities": ["fecha_hora", "mensaje"],
      "service_endpoint": "system-mcp-service"
    },
    "programar_alarma_condicional": {
      "description": "Programa alarma basada en condiciones externas",
      "required_entities": ["condicion"],
      "service_endpoint": "system-mcp-service"
    },
    "crear_github_issue": {
      "description": "Crea un issue en un repositorio de GitHub",
      "required_entities": ["titulo", "descripcion"],
      "service_endpoint": "github-mcp-service"
    }
  }
}
```

---

## Variables de Entorno Clave

```bash
# LLM Configuration
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
PRIMARY_LLM_MODEL=gpt-4
MOE_LLM_A_MODEL=gpt-4
MOE_LLM_B_MODEL=claude-3-sonnet-20240229
MOE_LLM_C_MODEL=gpt-3.5-turbo

# MoE Configuration
MOE_ENABLED=true
MOE_TIMEOUT_PER_VOTE=30
MOE_PARALLEL_VOTING=true
MOE_CONSENSUS_THRESHOLD=0.6
MOE_MAX_DEBATE_ROUNDS=1

# Intent Configuration
INTENT_CONFIG_FILE=classpath:config/intents.json
INTENT_HOT_RELOAD_ENABLED=true
INTENT_HOT_RELOAD_INTERVAL=30
INTENT_DEFAULT_CONFIDENCE_THRESHOLD=0.7
INTENT_DEFAULT_MAX_EXAMPLES_FOR_RAG=5

# Vector Store Configuration
VECTOR_STORE_TYPE=in-memory
VECTOR_STORE_COLLECTION=intent-examples
VECTOR_STORE_EMBEDDING_DIMENSION=1536
VECTOR_STORE_MAX_RESULTS=10
VECTOR_STORE_SIMILARITY_THRESHOLD=0.7
VECTOR_STORE_INIT_EXAMPLES=true
VECTOR_STORE_EXAMPLE_COUNT=5
```

---

## Estado Actual del Sistema

```
üéâ EPICS COMPLETADOS: 3/10 (30%)
‚úÖ Epic 1 - Arquitectura Base: COMPLETADO AL 100%
‚úÖ Epic 2 - Motor RAG: COMPLETADO AL 100%
‚úÖ Epic 3 - MoE Voting System: COMPLETADO AL 100%

üèóÔ∏è Infraestructura Base: OPERATIVA
üîç Motor RAG: FUNCIONANDO
üéØ Sistema de Votaci√≥n MoE: ACTIVO
üìä API REST: 103 endpoints operativos
‚úÖ Pruebas: 100% exitosas
üìù Documentaci√≥n: COMPLETA en archivos espec√≠ficos

üîÑ PR√ìXIMOS PASOS:
‚è≥ Epic 4 - Sistema Conversacional Inteligente
‚è≥ Epic 5 - Integraci√≥n Audio y Transcripci√≥n
‚è≥ Epic 6 - MCP Integration