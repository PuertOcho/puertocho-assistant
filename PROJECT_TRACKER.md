# PROJECT TRACKER â€“ IntentManagerMS LLM-RAG + MoE Implementation

> Ãšltima actualizaciÃ³n: 2025-01-27 (Actualizado con implementaciÃ³n real T1.1 y T1.2)
> 
> **Objetivo**: ImplementaciÃ³n completamente nueva de intentmanagerms usando arquitectura LLM-RAG + Mixture of Experts (MoE) para clasificaciÃ³n de intenciones escalable, conversaciÃ³n multivuelta inteligente y soporte nativo MCP.

---

## Leyenda de estados
- âœ… Completado
- ğŸ”„ En progreso  
- â³ Pendiente
- ğŸš§ Bloqueado

---

## Epic 1 â€“ Arquitectura Base y ConfiguraciÃ³n LLM-RAG

**DescripciÃ³n del Epic**: Establecer los fundamentos tÃ©cnicos del sistema LLM-RAG. Este Epic crea la infraestructura base necesaria para soportar mÃºltiples LLMs, almacenamiento vectorial para RAG, configuraciÃ³n dinÃ¡mica desde JSON, y el registro de acciones MCP. Es la base sobre la que se construyen todos los demÃ¡s Epics.

**Objetivos clave**: 
- Infraestructura Spring Boot funcional con LangChain4j
- GestiÃ³n configurable de mÃºltiples LLMs  
- Vector store operativo para embeddings RAG
- Sistema de configuraciÃ³n JSON hot-reload
- Registro centralizado de acciones MCP disponibles

| ID | DescripciÃ³n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T1.1 | Crear estructura base Java Spring Boot con dependencias LangChain4j | â€“ | âœ… |
| T1.2 | Implementar `LlmConfigurationService` para gestiÃ³n de mÃºltiples LLMs | T1.1 | âœ… |
| T1.3 | Crear `VectorStoreService` para embeddings RAG (Chroma/In-memory) | T1.1, T1.2 | â³ |
| T1.4 | DiseÃ±ar `IntentConfigManager` para cargar intenciones desde JSON dinÃ¡mico | T1.1, T1.2 | â³ |
| T1.5 | Implementar `McpActionRegistry` para acciones configurables | T1.1, T1.2 | â³ |

---

## ğŸ“‹ **IMPLEMENTACIÃ“N REAL COMPLETADA**

### **T1.1 âœ… - Estructura Base Java Spring Boot**
**Archivos Implementados:**
- âœ… `pom.xml` - Dependencias Spring Boot 3.2.1 + Spring Cloud + OpenAPI
- âœ… `IntentManagerApplication.java` - Clase principal Spring Boot
- âœ… `application.yml` - ConfiguraciÃ³n simplificada para configms
- âœ… `Dockerfile` - Multi-stage build optimizado
- âœ… `configms/intent-manager.yml` - ConfiguraciÃ³n centralizada

**ConfiguraciÃ³n Centralizada:**
```yaml
# configms/src/main/resources/configurations/intent-manager.yml
llm:
  primary:
    model: ${PRIMARY_LLM_MODEL:gpt-4}
    provider: ${PRIMARY_LLM_PROVIDER:openai}
    api-key: ${OPENAI_API_KEY:}
  timeout: ${LLM_TIMEOUT:30s}
  max-tokens: ${LLM_MAX_TOKENS:4096}
  temperature: ${LLM_TEMPERATURE:0.7}
```

### **T1.2 âœ… - LlmConfigurationService**
**Modelos de Dominio:**
- âœ… `LlmProvider` - Enum (OPENAI, ANTHROPIC, GOOGLE, AZURE, LOCAL)
- âœ… `LlmConfiguration` - ConfiguraciÃ³n completa con timeout, tokens, temperatura
- âœ… `LlmResponse` - Respuestas con metadatos y mÃ©tricas

**Servicios Implementados:**
- âœ… `LlmConfigurationService` - GestiÃ³n centralizada de mÃºltiples LLMs
- âœ… `LlmInitializationService` - InicializaciÃ³n automÃ¡tica con `@EventListener`
- âœ… `LlmConfigurationController` - 12 endpoints REST completos

**LLMs Configurados AutomÃ¡ticamente:**
```
âœ… primary: LLM Primario (gpt-4) - Peso: 1.0
âœ… fallback: LLM de Fallback (gpt-3.5-turbo) - Peso: 0.8  
âœ… moe-llm-a: Juez A - AnÃ¡lisis CrÃ­tico (gpt-4) - Peso: 1.0
âœ… moe-llm-c: Juez C - Practicidad de AcciÃ³n (gpt-3.5-turbo) - Peso: 0.9
âš ï¸ moe-llm-b: Omitido (API Key Anthropic no configurada)
```

**API REST Disponible:**
```bash
GET /api/v1/llm-config/statistics    # EstadÃ­sticas completas
GET /api/v1/llm-config/primary      # LLM primario
GET /api/v1/llm-config/voting       # LLMs para votaciÃ³n MoE
GET /api/v1/llm-config/{id}/health  # Health check individual
POST/PUT/DELETE /api/v1/llm-config  # CRUD completo
```

**Variables de Entorno Clave:**
```bash
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
MOE_ENABLED=true
PRIMARY_LLM_MODEL=gpt-4
MOE_LLM_A_MODEL=gpt-4
MOE_LLM_B_MODEL=claude-3-sonnet-20240229
MOE_LLM_C_MODEL=gpt-3.5-turbo
```

---

## Epic 2 â€“ Motor RAG para ClasificaciÃ³n de Intenciones

**DescripciÃ³n del Epic**: Implementar el motor de Retrieval Augmented Generation (RAG) que reemplaza completamente el sistema RASA/DU. Utiliza embeddings vectoriales para realizar few-shot learning con ejemplos de intenciones almacenados dinÃ¡micamente. El sistema busca ejemplos similares, construye prompts contextuales y classifica intenciones sin necesidad de entrenamiento tradicional.

**Objetivos clave**:
- ClasificaciÃ³n de intenciones sin entrenamiento previo
- Few-shot learning basado en ejemplos JSON
- Similarity search eficiente con embeddings
- Confidence scoring robusto y configurable
- Fallback inteligente para casos edge

| ID | DescripciÃ³n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T2.1 | Crear `RagIntentClassifier` con embeddings vectoriales para few-shot learning | T1.3, T1.4 | â³ |
| T2.2 | Implementar sistema de similarity search para ejemplos de intenciones | T2.1 | â³ |
| T2.3 | Desarrollar prompt engineering dinÃ¡mico con contexto RAG | T2.1 | â³ |
| T2.4 | AÃ±adir confidence scoring usando mÃºltiples mÃ©tricas | T2.2 | â³ |
| T2.5 | Crear fallback inteligente con degradaciÃ³n gradual | T2.4 | â³ |

## Epic 3 â€“ MoE Voting System (Sistema de VotaciÃ³n LLM)

**DescripciÃ³n del Epic**: Implementar sistema de votaciÃ³n donde mÃºltiples LLMs debaten brevemente la mejor acciÃ³n a tomar, reemplazando el concepto tradicional de "expertos especializados" por un "jurado de LLMs". Cada LLM vota independientemente y un motor de consenso determina la acciÃ³n final. Sistema completamente configurable que puede habilitarse/deshabilitarse via variables de entorno.

**Objetivos clave**:
- VotaciÃ³n simultÃ¡nea de 3 LLMs con roles especÃ­ficos
- Motor de consenso para procesar votos y decidir acciÃ³n final
- ConfiguraciÃ³n flexible: habilitar/deshabilitar via MOE_ENABLED
- Fallback a LLM Ãºnico cuando voting estÃ¡ deshabilitado
- Logging transparente del proceso de votaciÃ³n para debugging

| ID | DescripciÃ³n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T3.1 | Implementar `LlmVotingService` para sistema de debate entre mÃºltiples LLMs | T1.2 | â³ |
| T3.2 | Crear `VotingRound` donde 3 LLMs debaten brevemente la acciÃ³n a tomar | T3.1 | â³ |
| T3.3 | Desarrollar `ConsensusEngine` para procesar votos y llegar a decisiÃ³n final | T3.2 | â³ |
| T3.4 | Implementar configuraciÃ³n para habilitar/deshabilitar voting (MoE_ENABLED=true/false) | T3.3 | â³ |
| T3.5 | Crear fallback a LLM Ãºnico cuando voting estÃ¡ deshabilitado | T3.4 | â³ |

## Epic 4 â€“ Sistema Conversacional Inteligente + OrquestaciÃ³n de Subtareas

**DescripciÃ³n del Epic**: Desarrollar sistema conversacional avanzado que usa LLM para descomponer dinÃ¡micamente peticiones complejas en mÃºltiples subtareas ejecutables. NO usa configuraciones predefinidas, sino que el LLM analiza cada peticiÃ³n y identifica automÃ¡ticamente quÃ© MCPs/servicios necesita invocar. Mantiene estado de progreso y marca conversaciÃ³n como completada solo cuando todas las subtareas estÃ¡n ejecutadas exitosamente.

**Objetivos clave**:
- ConversaciÃ³n multivuelta con memoria contextual persistente
- Slot-filling automÃ¡tico usando LLM para preguntas dinÃ¡micas  
- **DescomposiciÃ³n dinÃ¡mica**: LLM identifica automÃ¡ticamente mÃºltiples acciones en una peticiÃ³n
- **Orquestador inteligente**: Ejecuta subtareas secuencial o paralelamente segÃºn dependencias detectadas
- **Estado de progreso**: Tracking automÃ¡tico hasta completion de todas las subtareas
- Manejo de anÃ¡foras y referencias contextuales

**Casos de uso - DescomposiciÃ³n DinÃ¡mica**:
- "Consulta el tiempo de Madrid y programa una alarma si va a llover" 
  â†’ LLM detecta: [consultar_tiempo + programar_alarma_condicional]
- "Crea un issue en GitHub sobre el weather bug y actualiza el estado en Taiga"
  â†’ LLM detecta: [crear_github_issue + actualizar_taiga_story]  
- "Enciende las luces del salÃ³n, pon mÃºsica relajante y ajusta la temperatura a 22Â°"
  â†’ LLM detecta: [encender_luces + reproducir_musica + ajustar_temperatura]

| ID | DescripciÃ³n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T4.1 | DiseÃ±ar `ConversationManager` con contexto LLM-powered | T2.1 | â³ |
| T4.2 | Implementar slot-filling automÃ¡tico usando LLM para preguntas dinÃ¡micas | T4.1 | â³ |
| T4.3 | Crear `EntityExtractor` basado en LLM para extracciÃ³n contextual | T4.1 | â³ |
| T4.4 | Desarrollar memoria conversacional con Redis para sesiones persistentes | T4.1 | â³ |
| T4.5 | Crear `DynamicSubtaskDecomposer` - LLM analiza peticiÃ³n y identifica mÃºltiples acciones automÃ¡ticamente | T4.1 | â³ |
| T4.6 | Implementar `TaskOrchestrator` para ejecuciÃ³n secuencial/paralela de subtareas detectadas dinÃ¡micamente | T4.5 | â³ |
| T4.7 | Desarrollar sistema de estado de progreso: tracking automÃ¡tico hasta completion de todas las subtareas | T4.6 | â³ |
| T4.8 | Implementar resoluciÃ³n de anÃ¡foras y referencias contextuales | T4.4 | â³ |

## Epic 5 â€“ IntegraciÃ³n Audio y TranscripciÃ³n

**DescripciÃ³n del Epic**: Integrar completamente el pipeline de audio desde recepciÃ³n hasta respuesta. Reemplaza la dependencia de servicios externos de transcripciÃ³n por integraciÃ³n directa con whisper-ms, maneja metadata de audio contextual (ubicaciÃ³n, temperatura, etc.), y soporta tanto entrada de texto como audio de forma unificada.

**Objetivos clave**:
- Pipeline completo: Audio â†’ TranscripciÃ³n â†’ ClasificaciÃ³n â†’ Respuesta
- IntegraciÃ³n directa con whisper-ms para transcripciÃ³n
- Soporte para metadata contextual de dispositivos (temperatura, ubicaciÃ³n)
- Manejo robusto de errores de transcripciÃ³n
- API unificada para texto y audio

| ID | DescripciÃ³n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T5.1 | Crear `AudioProcessingController` para recibir audio multipart/form-data | T1.1 | â³ |
| T5.2 | Implementar cliente `WhisperTranscriptionService` para whisper-ms | T5.1 | â³ |
| T5.3 | Desarrollar pipeline Audio â†’ TranscripciÃ³n â†’ ClasificaciÃ³n â†’ Respuesta | T5.2, T2.1 | â³ |
| T5.4 | AÃ±adir soporte para metadata de audio y contexto de dispositivo | T5.1 | â³ |
| T5.5 | Implementar manejo de errores de transcripciÃ³n con fallbacks | T5.2 | â³ |

## Epic 6 â€“ MCP (Model Context Protocol) Integration

| ID | DescripciÃ³n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T6.1 | Crear `McpClientService` para comunicaciÃ³n con servicios MCP externos | T1.5 | â³ |
| T6.2 | Implementar `TaigaMcpClient` reutilizando taiga-mcp-ms existente | T6.1 | â³ |
| T6.3 | Desarrollar `WeatherMcpClient` para consultas meteorolÃ³gicas | T6.1 | â³ |
| T6.4 | Crear `SystemMcpClient` para acciones de sistema (hora, alarmas, etc.) | T6.1 | â³ |
| T6.5 | AÃ±adir configuraciÃ³n JSON dinÃ¡mica para nuevos clientes MCP | T6.1 | â³ |

## Epic 7 â€“ API y Compatibilidad

| ID | DescripciÃ³n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T7.1 | DiseÃ±ar nuevos endpoints REST manteniendo compatibilidad con gatewayms | T5.1 | â³ |
| T7.2 | Crear `AssistantController` con soporte para audio + texto + contexto | T7.1, T5.3 | â³ |
| T7.3 | Implementar respuestas mÃºltiples: texto, audio TTS, acciones MCP | T7.2 | â³ |
| T7.4 | AÃ±adir endpoints de diagnÃ³stico y mÃ©tricas de rendimiento | T7.2 | â³ |
| T7.5 | Crear documentaciÃ³n OpenAPI/Swagger para nuevos endpoints | T7.1 | â³ |

## Epic 8 â€“ ConfiguraciÃ³n JSON DinÃ¡mica

| ID | DescripciÃ³n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T8.1 | DiseÃ±ar esquema JSON para definiciÃ³n de intenciones y ejemplos | T1.4 | â³ |
| T8.2 | Crear esquema JSON para configuraciÃ³n de expertos MoE | T3.1 | â³ |
| T8.3 | Implementar esquema JSON para acciones MCP y parÃ¡metros | T1.5 | â³ |
| T8.4 | AÃ±adir hot-reload de configuraciones sin reiniciar el servicio | T8.1, T8.2, T8.3 | â³ |
| T8.5 | Crear validaciÃ³n de esquemas JSON con feedback detallado | T8.4 | â³ |

## Epic 9 â€“ Testing y EvaluaciÃ³n

| ID | DescripciÃ³n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T9.1 | Crear suite de tests unitarios para todos los componentes | Todas | â³ |
| T9.2 | Implementar tests de integraciÃ³n para flujo completo audioâ†’respuesta | T5.3, T7.3 | â³ |
| T9.3 | Desarrollar benchmarks de rendimiento vs sistema RASA/DU anterior | T2.1, T3.5 | â³ |
| T9.4 | Crear tests de escalabilidad para adiciÃ³n dinÃ¡mica de intenciones | T8.4 | â³ |
| T9.5 | Implementar mÃ©tricas de calidad conversacional y accuracy | T4.2, T4.5 | â³ |

## Epic 10 â€“ Despliegue y ConfiguraciÃ³n

| ID | DescripciÃ³n | Dependencias | Estado |
|----|-------------|--------------|--------|
| T10.1 | Actualizar Dockerfile para nuevas dependencias LangChain4j | T1.1 | â³ |
| T10.2 | Configurar variables de entorno para LLMs y servicios externos | T1.2 | â³ |
| T10.3 | Crear archivos JSON de configuraciÃ³n por defecto para demo | T8.1, T8.2, T8.3 | â³ |
| T10.4 | Integrar con docker-compose existente manteniendo compatibilidad | T10.1 | â³ |
| T10.5 | Documentar migraciÃ³n desde sistema anterior | Todas | â³ |

---

## Roadmap de ImplementaciÃ³n

### âœ… **Fase 1: Fundamentos (COMPLETADA)**
- âœ… **Epic 1**: Arquitectura Base (T1.1, T1.2 completados)
- â³ **Epic 1**: Resto de tareas (T1.3, T1.4, T1.5)
- â³ **Epic 2**: Motor RAG bÃ¡sico
- â³ **Epic 5**: IntegraciÃ³n Audio (bÃ¡sica)

### ğŸ”„ **Fase 2: Inteligencia (EN PROGRESO)**
- âœ… **Epic 3**: MoE Architecture (Base T1.2 completada)
- â³ **Epic 4**: Sistema Conversacional
- â³ **Epic 8**: ConfiguraciÃ³n JSON

### â³ **Fase 3: IntegraciÃ³n**
- â³ **Epic 6**: MCP Integration
- â³ **Epic 7**: API y Compatibilidad
- â³ **Epic 9**: Testing bÃ¡sico

### â³ **Fase 4: OptimizaciÃ³n**
- â³ **Epic 9**: Testing completo y evaluaciÃ³n
- â³ **Epic 10**: Despliegue y documentaciÃ³n

### **ğŸ“Š Progreso Actual:**
- **Epic 1**: 2/5 tareas completadas (40%)
- **Epic 2**: 0/5 tareas completadas (0%)
- **Epic 3**: Base preparada, pendiente implementaciÃ³n completa
- **Total General**: 2/50 tareas completadas (4%)

---

## Arquitectura Implementada vs Objetivo

### **âœ… IMPLEMENTADO (T1.1 + T1.2)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Config    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    REST API    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CONFIGMS      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  INTENTMGR-V2   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  LLM CONFIG     â”‚
â”‚ (Centralizada)  â”‚              â”‚ (Spring Boot)   â”‚                â”‚   SERVICE       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚                                â”‚
                                          â–¼                                â–¼
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚ LLM INIT SERVICEâ”‚              â”‚ LLM CONFIG API  â”‚
                                 â”‚ (Auto Setup)    â”‚              â”‚ (12 endpoints)  â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚                                â”‚
                                          â–¼                                â–¼
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚   LLM POOL      â”‚              â”‚   LLM HEALTH    â”‚
                                 â”‚ (4 LLMs Ready)  â”‚              â”‚   (Monitoring)  â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PRIMARY LLM   â”‚              â”‚   MOE LLM-A     â”‚    â”‚   MOE LLM-C     â”‚
â”‚   (GPT-4)       â”‚              â”‚ (GPT-4 Juez A)  â”‚    â”‚(GPT-3.5 Juez C) â”‚
â”‚   Peso: 1.0     â”‚              â”‚   Peso: 1.0     â”‚    â”‚   Peso: 0.9     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                â”‚                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ FALLBACK LLM    â”‚
                â”‚ (GPT-3.5-Turbo) â”‚
                â”‚   Peso: 0.8     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **ğŸ¯ OBJETIVO COMPLETO (Futuras Tareas)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Audio     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    JSON    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WHISPER-MS     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  GATEWAY-MS     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ INTENTMGR-V2    â”‚
â”‚  (TranscripciÃ³n)â”‚              â”‚  (Routing)      â”‚            â”‚ (LLM-RAG+MoE)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                          â”‚
                                                                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VECTOR STORE    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   RAG ENGINE    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  EXPERT ROUTER  â”‚
â”‚ (Embeddings)    â”‚              â”‚ (Similarity)    â”‚            â”‚  (MoE Selection) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                          â”‚
                                                                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     LLM-A       â”‚              â”‚     LLM-B       â”‚    â”‚     LLM-C       â”‚
â”‚   (GPT-4)       â”‚              â”‚  (Claude-3)     â”‚    â”‚ (GPT-3.5-Turbo) â”‚
â”‚   "Voto: X"     â”‚              â”‚   "Voto: Y"     â”‚    â”‚   "Voto: Z"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                â”‚                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ CONSENSUS ENGINEâ”‚
                â”‚ (Procesa votos) â”‚
                â”‚ DecisiÃ³n final  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONVERSATION    â”‚              â”‚   MCP ACTIONS   â”‚    â”‚   SINGLE LLM    â”‚
â”‚   MANAGER       â”‚              â”‚ (Taiga/Weather) â”‚    â”‚   (Fallback)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ConfiguraciÃ³n de Ejemplo

### intents.json
```json
{
  "intents": {
    "consultar_tiempo": {
      "examples": [
        "Â¿quÃ© tiempo hace?",
        "dime el clima de hoy",
        "cÃ³mo estÃ¡ el tiempo"
      ],
      "required_entities": ["ubicacion"],
      "mcp_action": "weather_query",
      "expert_domain": "general",
      "slot_filling_questions": {
        "ubicacion": "Â¿De quÃ© ciudad quieres consultar el tiempo?"
      }
    },
    "encender_luz": {
      "examples": [
        "enciende la luz",
        "prende la lÃ¡mpara",
        "ilumina la habitaciÃ³n"
      ],
      "required_entities": ["lugar"],
      "mcp_action": "smart_home_light_on",
      "expert_domain": "smart_home",
      "slot_filling_questions": {
        "lugar": "Â¿En quÃ© habitaciÃ³n quieres encender la luz?"
      }
    }
  }
}
```

### moe_voting.json (Implementado)
```json
{
  "voting_system": {
    "enabled": "${MOE_ENABLED:true}",
    "max_debate_rounds": 1,
    "consensus_threshold": 0.6,
    "llm_participants": [
      {
        "id": "moe-llm-a", 
        "model": "gpt-4",
        "api_key": "${OPENAI_API_KEY}",
        "role": "Juez A - AnÃ¡lisis crÃ­tico y evaluaciÃ³n detallada",
        "weight": 1.0,
        "temperature": 0.3,
        "max_tokens": 2048
      },
      {
        "id": "moe-llm-b",
        "model": "claude-3-sonnet-20240229", 
        "api_key": "${ANTHROPIC_API_KEY}",
        "role": "Juez B - AnÃ¡lisis de contexto conversacional y coherencia",
        "weight": 1.0,
        "temperature": 0.3,
        "max_tokens": 2048
      },
      {
        "id": "moe-llm-c",
        "model": "gpt-3.5-turbo",
        "api_key": "${OPENAI_API_KEY}",
        "role": "Juez C - EvaluaciÃ³n de practicidad y factibilidad de acciones",
        "weight": 0.9,
        "temperature": 0.3,
        "max_tokens": 2048
      }
    ],
    "fallback_llm": {
      "id": "primary",
      "model": "gpt-4",
      "api_key": "${OPENAI_API_KEY}",
      "used_when": "voting_disabled_or_failed"
    }
  }
}
```

### Ejemplo de Flujo de VotaciÃ³n Simple
```
Usuario: "Enciende la luz del salÃ³n"

LLM-A Voto: "ACCIÃ“N: encender_luz, ENTIDADES: {lugar: 'salÃ³n'}, CONFIANZA: 0.9"
LLM-B Voto: "ACCIÃ“N: encender_luz, ENTIDADES: {lugar: 'salÃ³n'}, CONFIANZA: 0.85" 
LLM-C Voto: "ACCIÃ“N: encender_luz, ENTIDADES: {lugar: 'salÃ³n'}, CONFIANZA: 0.8"

CONSENSO: encender_luz (3/3 votos) â†’ EJECUTAR ACCIÃ“N MCP
```

### Ejemplo de DescomposiciÃ³n DinÃ¡mica por LLM
```
Usuario: "Consulta el tiempo de Madrid y programa una alarma si va a llover"

PASO 1 - ANÃLISIS LLM:
LLM Prompt: "Analiza esta peticiÃ³n y lista las acciones MCP necesarias: 'Consulta el tiempo de Madrid y programa una alarma si va a llover'"

LLM Response: {
  "subtasks": [
    {
      "action": "consultar_tiempo",
      "entities": {"ubicacion": "Madrid"},
      "description": "Consultar informaciÃ³n meteorolÃ³gica de Madrid"
    },
    {
      "action": "programar_alarma_condicional", 
      "entities": {"condicion": "si_llueve", "ubicacion": "Madrid"},
      "description": "Programar alarma basada en condiciÃ³n meteorolÃ³gica",
      "depends_on_result": "consultar_tiempo"
    }
  ]
}

PASO 2 - EJECUCIÃ“N ORQUESTADA:
- Estado inicial: PENDING [consultar_tiempo, programar_alarma_condicional]
- Ejecuta: consultar_tiempo(Madrid) â†’ Resultado: "Lluvia probable 70%"  
- Estado: PENDING [programar_alarma_condicional], COMPLETED [consultar_tiempo]
- Ejecuta: programar_alarma_condicional(condicion="si_llueve") â†’ Resultado: "Alarma creada"
- Estado: COMPLETED [consultar_tiempo, programar_alarma_condicional]
- CONVERSACIÃ“N COMPLETADA: "En Madrid hay 70% probabilidad de lluvia. He programado una alarma para avisarte."
```

### ConfiguraciÃ³n de MCPs Disponibles (mcp_registry.json)
```json
{
  "available_mcps": {
    "consultar_tiempo": {
      "description": "Consulta informaciÃ³n meteorolÃ³gica de una ubicaciÃ³n",
      "required_entities": ["ubicacion"],
      "service_endpoint": "weather-mcp-service"
    },
    "programar_alarma": {
      "description": "Programa una alarma para una fecha/hora especÃ­fica", 
      "required_entities": ["fecha_hora", "mensaje"],
      "service_endpoint": "system-mcp-service"
    },
    "programar_alarma_condicional": {
      "description": "Programa alarma basada en condiciones externas",
      "required_entities": ["condicion"],
      "service_endpoint": "system-mcp-service"
    },
    "crear_github_issue": {
      "description": "Crea un issue en un repositorio de GitHub",
      "required_entities": ["titulo", "descripcion"],
      "service_endpoint": "github-mcp-service"
    }
  }
}
```